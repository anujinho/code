{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import warnings\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.distributions as D\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from sklearn.manifold import TSNE\n",
    "# from torchvision.datasets import MNIST\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# For DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 2024 has been set.\n"
     ]
    }
   ],
   "source": [
    "set_seed(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUTS_DIR = 'outputs_toy_final'\n",
    "\n",
    "if not Path.exists(Path(OUTPUTS_DIR)):\n",
    "    Path.mkdir(Path(OUTPUTS_DIR), exist_ok=True, parents=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Class\n",
    "\n",
    "Reference: https://deeplearning.neuromatch.io/tutorials/W2D4_GenerativeModels/student/W2D4_Tutorial2.html#custom-gaussian-mixture-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixture:\n",
    "\n",
    "  def __init__(self, mus, covs, weights, device='cuda'):\n",
    "    \"\"\"\n",
    "    mus: a list of K 1d np arrays (D,)\n",
    "    covs: a list of K 2d np arrays (D, D)\n",
    "    weights: a list or array of K unnormalized non-negative weights, signifying the possibility of sampling from each branch.\n",
    "      They will be normalized to sum to 1. If they sum to zero, it will err.\n",
    "    \"\"\"\n",
    "    self.n_component = len(mus)\n",
    "    self.mus = mus\n",
    "    self.a = covs\n",
    "    self.device = device\n",
    "    self.precs = [torch.from_numpy(np.linalg.inv((cov * torch.eye(2).cuda()).cpu().numpy())).to(cov.device) for cov in covs]\n",
    "    self.weights = np.array(weights)\n",
    "    self.norm_weights = self.weights / self.weights.sum()\n",
    "    self.RVs = []\n",
    "    for i in range(len(mus)):\n",
    "      self.RVs.append(D.Independent(D.Normal(mus[i], covs[i]), 1))\n",
    "    self.dim = len(mus[0])\n",
    "\n",
    "  def compute_mean(self):\n",
    "\n",
    "    mu_bar = 0.0\n",
    "    for weight, mu in zip(self.norm_weights, self.mus):\n",
    "      mu_bar += weight * mu\n",
    "\n",
    "    return mu_bar\n",
    "\n",
    "  def compute_variance(self):\n",
    "\n",
    "    mu_bar = self.compute_mean()\n",
    "    var = None\n",
    "    for weight, mu, cov in zip(self.norm_weights, self.mus, self.covs):\n",
    "\n",
    "      assert cov[0] == cov[1]\n",
    "\n",
    "      covmat = cov[0]**2 * torch.eye(2).to(cov.device)\n",
    "\n",
    "      temp = weight * (covmat + torch.mm((mu - mu_bar).unsqueeze(0).T, (mu - mu_bar).unsqueeze(0)))\n",
    "      var = temp if var is None else var + temp\n",
    "\n",
    "    return var\n",
    "\n",
    "  def add_component(self, mu, cov, weight=1):\n",
    "    self.mus.append(mu)\n",
    "    self.covs.append(cov)\n",
    "    self.precs.append(np.linalg.inv(cov.cpu().numpy()))\n",
    "    self.RVs.append(D.Independent(D.Normal(mu, cov), 1))\n",
    "    self.weights.append(weight)\n",
    "    self.norm_weights = self.weights / self.weights.sum()\n",
    "    self.n_component += 1\n",
    "\n",
    "  def pdf_decompose(self, x):\n",
    "    \"\"\"\n",
    "      probability density (PDF) at $x$.\n",
    "    \"\"\"\n",
    "    component_pdf = []\n",
    "    prob = None\n",
    "    for weight, RV in zip(self.norm_weights, self.RVs):\n",
    "        pdf = weight * RV.log_prob(x).exp()\n",
    "        prob = pdf if prob is None else (prob + pdf)\n",
    "        component_pdf.append(pdf)\n",
    "    component_pdf = np.array(component_pdf)\n",
    "    return prob, component_pdf\n",
    "\n",
    "  def pdf(self, x):\n",
    "    \"\"\"\n",
    "      probability density (PDF) at $x$.\n",
    "    \"\"\"\n",
    "    isnumpy = False\n",
    "    if type(x) is np.ndarray:\n",
    "      isnumpy = True\n",
    "      x = torch.from_numpy(x).to(self.mus[0].device)\n",
    "\n",
    "    prob = None\n",
    "    for weight, RV in zip(self.norm_weights, self.RVs):\n",
    "        pdf = weight * RV.log_prob(x).exp()\n",
    "        prob = pdf if prob is None else (prob + pdf)\n",
    "\n",
    "    if isnumpy:\n",
    "      prob = prob.cpu().numpy()\n",
    "\n",
    "    return prob\n",
    "  \n",
    "  def scaled_pdf(self, x, scale):\n",
    "    \"\"\"\n",
    "      probability density (PDF) at $x$.\n",
    "    \"\"\"\n",
    "    isnumpy = False\n",
    "    if type(x) is np.ndarray:\n",
    "      isnumpy = True\n",
    "      x = torch.from_numpy(x).to(self.mus[0].device)\n",
    "\n",
    "    scale = torch.tensor(scale).to(self.mus[0].device)\n",
    "    scale = torch.sqrt(scale)\n",
    "\n",
    "    scaled_x = scale * x\n",
    "    prob = None\n",
    "    for idx in range(self.n_component):\n",
    "      component = D.Independent(D.Normal(scale * self.mus[idx], self.a[idx]), 1)\n",
    "      pdf = self.norm_weights[idx] * component.log_prob(scaled_x).exp()\n",
    "      prob = pdf if prob is None else (prob + pdf)\n",
    "\n",
    "    if isnumpy:\n",
    "      prob = prob.cpu().numpy()\n",
    "\n",
    "    return prob\n",
    "\n",
    "  def score(self, x):\n",
    "    \"\"\"\n",
    "    Compute the score $\\nabla_x \\log p(x)$ for the given $x$.\n",
    "    \"\"\"\n",
    "    isnumpy = False\n",
    "    if type(x) is np.ndarray:\n",
    "      isnumpy = True\n",
    "      x = torch.from_numpy(x).to(torch.float).to(self.mus[0].device)\n",
    "\n",
    "    component_pdf = np.array([rv.log_prob(x).exp().cpu().numpy() for rv in self.RVs])\n",
    "    component_pdf = torch.from_numpy(component_pdf).T\n",
    "\n",
    "    weighted_compon_pdf = component_pdf * self.norm_weights[np.newaxis, :]\n",
    "    participance = weighted_compon_pdf / weighted_compon_pdf.sum(axis=1, keepdims=True)\n",
    "    participance = participance.to(self.mus[0].device)\n",
    "\n",
    "    scores = torch.zeros_like(x)\n",
    "    for i in range(self.n_component):\n",
    "      gradvec = - (x - self.mus[i]) @ self.precs[i]\n",
    "      scores += participance[:, i:i+1] * gradvec\n",
    "\n",
    "    if isnumpy:\n",
    "      scores = scores.cpu().numpy()\n",
    "\n",
    "    return scores\n",
    "\n",
    "  def score_decompose(self, x):\n",
    "    \"\"\"\n",
    "    Compute the grad to each branch for the score $\\nabla_x \\log p(x)$ for the given $x$.\n",
    "    \"\"\"\n",
    "    component_pdf = np.array([rv.log_prob(x).exp() for rv in self.RVs]).T\n",
    "    weighted_compon_pdf = component_pdf * self.norm_weights[np.newaxis, :]\n",
    "    participance = weighted_compon_pdf / weighted_compon_pdf.sum(axis=1, keepdims=True)\n",
    "\n",
    "    gradvec_list = []\n",
    "    for i in range(self.n_component):\n",
    "      gradvec = - (x - self.mus[i]) @ self.precs[i]\n",
    "      gradvec_list.append(gradvec)\n",
    "      # scores += participance[:, i:i+1] * gradvec\n",
    "\n",
    "    return gradvec_list, participance\n",
    "\n",
    "  def sample(self, N):\n",
    "    \"\"\" Draw N samples from Gaussian mixture\n",
    "    Procedure:\n",
    "      Draw N samples from each Gaussian\n",
    "      Draw N indices, according to the weights.\n",
    "      Choose sample between the branches according to the indices.\n",
    "    \"\"\"\n",
    "    rand_component = np.random.choice(self.n_component, size=N, p=self.norm_weights)\n",
    "    all_samples = np.array([rv.sample((N,)).cpu().numpy() for rv in self.RVs])\n",
    "    gmm_samps = all_samples[rand_component, np.arange(N),:]\n",
    "    return gmm_samps, rand_component, all_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian mixture\n",
    "mu1 = torch.tensor([2.0, 8.0]).to('cuda')\n",
    "Cov1 = torch.tensor([1.0, 1.0]).to('cuda')\n",
    "\n",
    "mu2 = torch.tensor([5.0, 2.0]).to('cuda')\n",
    "Cov2 = torch.tensor([1.0, 1.0]).to('cuda')\n",
    "\n",
    "mu3 = torch.tensor([8.0, 8.0]).to('cuda')\n",
    "Cov3 = torch.tensor([1.0, 1.0]).to('cuda')\n",
    "\n",
    "gmm = GaussianMixture([mu1, mu2, mu3],[Cov1, Cov2, Cov3], [1.0, 1.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_pdf_contour_plot(gmm, xlim=None,ylim=None,ticks=100,logprob=False,label=None,**kwargs):\n",
    "    if xlim is None:\n",
    "        xlim = plt.xlim()\n",
    "    if ylim is None:\n",
    "        ylim = plt.ylim()\n",
    "    xx, yy = np.meshgrid(np.linspace(*xlim, ticks), np.linspace(*ylim, ticks))\n",
    "    pdf = gmm.pdf(np.dstack((xx,yy)))\n",
    "    if logprob:\n",
    "        pdf = np.log(pdf)\n",
    "    plt.contour(xx, yy, pdf, **kwargs,)\n",
    "\n",
    "# @title Visualize log density\n",
    "show_samples = True  # @param {type:\"boolean\"}\n",
    "np.random.seed(42)\n",
    "gmm_samples, _, _ = gmm.sample(2000)\n",
    "plt.figure(figsize=[8, 8])\n",
    "plt.axis([0,10,0,10])\n",
    "plt.scatter(gmm_samples[:, 0],\n",
    "            gmm_samples[:, 1],\n",
    "            s=10,\n",
    "            alpha=0.4 if show_samples else 0.0)\n",
    "gmm_pdf_contour_plot(gmm, cmap=\"Greys\", levels=20, logprob=True)\n",
    "plt.title(\"log density of gaussian mixture $\\log p(x)$\")\n",
    "# plt.axis(\"image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualize Score\n",
    "set_seed(2024)\n",
    "\n",
    "basedist = D.Independent(D.Normal(torch.tensor([5.0, 6.0]), torch.ones(2)), 1)\n",
    "gmm_samps_few, _, _ = gmm.sample(300)\n",
    "gmm_samps_few = np.concatenate([gmm_samps_few, basedist.sample((100,)).cpu().numpy()], axis=0)\n",
    "scorevecs_few = gmm.score(gmm_samps_few)\n",
    "# gradvec_list, participance = gmm.score_decompose(gmm_samps_few)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quiver_plot(pnts, vecs, *args, **kwargs):\n",
    "  plt.quiver(pnts[:, 0], pnts[:,1], vecs[:, 0], vecs[:, 1], *args, **kwargs)\n",
    "\n",
    "# @title Score for Gaussian mixture\n",
    "plt.figure(figsize=[8, 8])\n",
    "quiver_plot(gmm_samps_few, scorevecs_few,\n",
    "            color=\"black\", scale=25, alpha=0.7, width=0.003,\n",
    "            label=\"score of GMM\")\n",
    "gmm_pdf_contour_plot(gmm, cmap=\"Greys\")\n",
    "plt.title(\"Score vector field $\\\\nabla\\log p(x)$ for Gaussian Mixture\")\n",
    "plt.axis(\"image\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenarios - Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Combined plots\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Prior\n",
    "mu_b_1 = torch.tensor([5.0, 3.0]).to('cuda')\n",
    "Cov_b_1 = torch.tensor([2.0, 2.0]).to('cuda')\n",
    "\n",
    "mu_b_2 = torch.tensor([3.0, 7.0]).to('cuda')\n",
    "Cov_b_2 = torch.tensor([2.0, 2.0]).to('cuda')\n",
    "\n",
    "mu_b_3 = torch.tensor([7.0, 7.0]).to('cuda')\n",
    "Cov_b_3 = torch.tensor([2.0, 2.0]).to('cuda')\n",
    "\n",
    "gmm_b = GaussianMixture([mu_b_1, mu_b_2, mu_b_3],[Cov_b_1, Cov_b_2, Cov_b_3], [1.0, 1.0, 1.0])\n",
    "\n",
    "# Reward: Setup I\n",
    "mu_r1 = torch.tensor([5.0, 3.0]).to('cuda')\n",
    "Cov_r1 = torch.tensor([1.0, 1.0]).to('cuda')\n",
    "\n",
    "gmm_r1 = GaussianMixture([mu_r1],[Cov_r1], [1.0])\n",
    "\n",
    "# Reward: Setup II\n",
    "mu_r2 = torch.tensor([14.0, 3.0]).to('cuda')\n",
    "Cov_r2 = torch.tensor([1.0, 1.0]).to('cuda')\n",
    "\n",
    "gmm_r2 = GaussianMixture([mu_r2],[Cov_r2], [1.0])\n",
    "\n",
    "def z_func(x, y, func, scale=None, posterior=None):\n",
    "    h, w = x.shape\n",
    "    x_flat = x.reshape(-1)\n",
    "    y_flat = y.reshape(-1)\n",
    "    x1 = np.column_stack((y_flat, x_flat))\n",
    "\n",
    "    if scale is None:\n",
    "        results = np.apply_along_axis(func, 1, x1)\n",
    "    else:\n",
    "        results = np.apply_along_axis(func, 1, x1, scale=scale, posterior=posterior)\n",
    "        \n",
    "    return results.reshape(h,w).T\n",
    "\n",
    "def posterior(x, scale, posterior):\n",
    "    return gmm_b.pdf(x) * posterior.scaled_pdf(x, scale=scale)\n",
    "\n",
    "# x = np.arange(15.5, -6, -0.1) # -5, 15\n",
    "x = np.arange(-5, 16.5, 0.1)\n",
    "y = np.arange(-5, 16.5, 0.1)\n",
    "X,Y = np.meshgrid(x, y) # grid of point\n",
    "\n",
    "IS_OOD = True # Setup I: False, Setup II: True\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "scale_factor = 0.1\n",
    "\n",
    "out_b = z_func(X, Y, gmm_b.pdf)\n",
    "cnt1 = ax[0].contour(X,Y,out_b, label=r'$p(x)$', cmap='Blues')\n",
    "\n",
    "if not IS_OOD:\n",
    "    out_pr1 = z_func(X, Y, gmm_r1.pdf)\n",
    "    cnt2 = ax[0].contour(X,Y,out_pr1, label=r'$exp(r(x))$', cmap='Reds')\n",
    "    ax[0].set_xticks(x[0::50])\n",
    "    ax[0].set_yticks(y[0::50])\n",
    "\n",
    "    out_post1 = z_func(X, Y, posterior, scale=scale_factor, posterior=gmm_r1)\n",
    "    out_post1 /= np.sum(out_post1)\n",
    "    cnt3 = ax[1].contour(X,Y,out_post1, label=r'$p^*(x) \\propto p(x)exp(r(x))$', cmap='Greens')\n",
    "    ax[1].set_xticks(x[0::50])\n",
    "    ax[1].set_yticks(y[0::50])\n",
    "\n",
    "else:\n",
    "    out_pr2 = z_func(X, Y, gmm_r2.pdf)\n",
    "    cnt2 = ax[0].contour(X,Y,out_pr2, label=r'$exp(r(x))$', cmap='Reds')\n",
    "    ax[0].set_xticks(x[0::50])\n",
    "    ax[0].set_yticks(y[0::50])\n",
    "\n",
    "    out_post2 = z_func(X, Y, posterior, scale=scale_factor, posterior=gmm_r2)\n",
    "    out_post2 /= np.sum(out_post2)\n",
    "    cnt3 = ax[1].contour(X,Y,out_post2, label=r'$p^*(x) \\propto p(x)exp(r(x))$', cmap='Greens')\n",
    "    ax[1].set_xticks(x[0::50])\n",
    "    ax[1].set_yticks(y[0::50])\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "line_b = Line2D([0], [0], label=r'Prior: $p(x)$', color=cnt1.collections[8].get_edgecolor()[0], lw=2)\n",
    "line_pr = Line2D([0], [0], label=r'Reward: $p(r|x)$', color=cnt2.collections[7].get_edgecolor()[0], lw=2)\n",
    "line_post = Line2D([0], [0], label=r'Posterior: $p(x|r) \\propto p(x)p(r|x)$', color=cnt3.collections[6].get_edgecolor()[0], lw=2)\n",
    "\n",
    "handles.extend([line_b, line_pr, line_post])\n",
    "fig.legend(handles=handles, bbox_to_anchor=(0.97, 1.09), ncols=3, fontsize=16, frameon=False)\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(f'{OUTPUTS_DIR}/toysetup_{\"sc1\" if not IS_OOD else \"sc2\"}.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Setup\n",
    "\n",
    "Reference: https://github.com/tanelp/tiny-diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 1000\n",
    "RESAMPLE_NOISE = False # CAUTION: IT WILL OVERWRITE THE NOISE SAMPLES!!!!\n",
    "\n",
    "if RESAMPLE_NOISE:\n",
    "    print('Sampling noise')\n",
    "    NOISE_SAMPLES = torch.randn(NUM_SAMPLES, 2)\n",
    "    torch.save(NOISE_SAMPLES, f'{OUTPUTS_DIR}/noise.pt')\n",
    "else:\n",
    "    print('Loading noise samples')\n",
    "    NOISE_SAMPLES = torch.load(f'{OUTPUTS_DIR}/noise.pt')\n",
    "\n",
    "NOISE_SAMPLES = NOISE_SAMPLES.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(self, size: int, scale: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x * self.scale\n",
    "        half_size = self.size // 2\n",
    "        emb = torch.log(torch.Tensor([10000.0])) / (half_size - 1)\n",
    "        emb = torch.exp(-emb * torch.arange(half_size)).to(x.device)\n",
    "        emb = x.unsqueeze(-1) * emb.unsqueeze(0)\n",
    "        emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=-1)\n",
    "        return emb\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "class LinearEmbedding(nn.Module):\n",
    "    def __init__(self, size: int, scale: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x / self.size * self.scale\n",
    "        return x.unsqueeze(-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "class LearnableEmbedding(nn.Module):\n",
    "    def __init__(self, size: int):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.linear = nn.Linear(1, size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.linear(x.unsqueeze(-1).float() / self.size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "class IdentityEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x.unsqueeze(-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "class ZeroEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x.unsqueeze(-1) * 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, size: int, type: str, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        if type == \"sinusoidal\":\n",
    "            self.layer = SinusoidalEmbedding(size, **kwargs)\n",
    "        elif type == \"linear\":\n",
    "            self.layer = LinearEmbedding(size, **kwargs)\n",
    "        elif type == \"learnable\":\n",
    "            self.layer = LearnableEmbedding(size)\n",
    "        elif type == \"zero\":\n",
    "            self.layer = ZeroEmbedding()\n",
    "        elif type == \"identity\":\n",
    "            self.layer = IdentityEmbedding()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown positional embedding type: {type}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer(x)\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ff = nn.Linear(size, size)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x + self.act(self.ff(x))\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size: int = 128, hidden_layers: int = 3, emb_size: int = 128,\n",
    "                 time_emb: str = \"sinusoidal\", input_emb: str = \"sinusoidal\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_mlp = PositionalEmbedding(emb_size, time_emb)\n",
    "        self.input_mlp1 = PositionalEmbedding(emb_size, input_emb, scale=25.0)\n",
    "        self.input_mlp2 = PositionalEmbedding(emb_size, input_emb, scale=25.0)\n",
    "\n",
    "        concat_size = len(self.time_mlp.layer) + \\\n",
    "            len(self.input_mlp1.layer) + len(self.input_mlp2.layer)\n",
    "        layers = [nn.Linear(concat_size, hidden_size), nn.GELU()]\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(Block(hidden_size))\n",
    "        layers.append(nn.Linear(hidden_size, 2))\n",
    "        self.joint_mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x1_emb = self.input_mlp1(x[:, 0]).to(x.device)\n",
    "        x2_emb = self.input_mlp2(x[:, 1]).to(x.device)\n",
    "        t_emb = self.time_mlp(t).to(x.device)\n",
    "        x = torch.cat((x1_emb, x2_emb, t_emb), dim=-1)\n",
    "        x = self.joint_mlp(x)\n",
    "        return x\n",
    "\n",
    "class Conditional_MLP(nn.Module):\n",
    "    def __init__(self, hidden_size: int = 128, hidden_layers: int = 3, emb_size: int = 128,\n",
    "                 time_emb: str = \"sinusoidal\", input_emb: str = \"sinusoidal\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_mlp = PositionalEmbedding(emb_size, time_emb)\n",
    "        self.input_mlp1 = PositionalEmbedding(emb_size, input_emb, scale=25.0)\n",
    "        self.input_mlp2 = PositionalEmbedding(emb_size, input_emb, scale=25.0)\n",
    "        self.class_mlp = PositionalEmbedding(emb_size, input_emb, scale=25.0)\n",
    "\n",
    "        concat_size = len(self.time_mlp.layer) + \\\n",
    "            len(self.input_mlp1.layer) + len(self.input_mlp2.layer) + len(self.class_mlp.layer)\n",
    "        layers = [nn.Linear(concat_size, hidden_size), nn.GELU()]\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(Block(hidden_size))\n",
    "        layers.append(nn.Linear(hidden_size, 2))\n",
    "        self.joint_mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, y, t):\n",
    "        # print(f'{x.shape} {y.shape} {t.shape}')\n",
    "        x1_emb = self.input_mlp1(x[:, 0]).to(x.device)\n",
    "        x2_emb = self.input_mlp2(x[:, 1]).to(x.device)\n",
    "        t_emb = self.time_mlp(t).to(x.device)\n",
    "        y_emb = self.class_mlp(y[:,0]).to(x.device)\n",
    "        # print(f'{x1_emb.shape} {x2_emb.shape} {t_emb.shape} {y_emb.shape}')\n",
    "        x = torch.cat((x1_emb, x2_emb, t_emb, y_emb), dim=-1)\n",
    "        x = self.joint_mlp(x)\n",
    "        return x\n",
    "\n",
    "class NoiseScheduler():\n",
    "    def __init__(self,\n",
    "                 num_timesteps=1000,\n",
    "                 beta_start=0.0001,\n",
    "                 beta_end=0.02,\n",
    "                 beta_schedule=\"linear\"):\n",
    "\n",
    "        self.num_timesteps = num_timesteps\n",
    "        if beta_schedule == \"linear\":\n",
    "            self.betas = torch.linspace(\n",
    "                beta_start, beta_end, num_timesteps, dtype=torch.float32)\n",
    "        elif beta_schedule == \"quadratic\":\n",
    "            self.betas = torch.linspace(\n",
    "                beta_start ** 0.5, beta_end ** 0.5, num_timesteps, dtype=torch.float32) ** 2\n",
    "\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.sqrt_alphas =  self.alphas ** 0.5\n",
    "        self.sqrt_betas = self.betas ** 0.5\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
    "        self.alphas_cumprod_prev = F.pad(\n",
    "            self.alphas_cumprod[:-1], (1, 0), value=1.)\n",
    "\n",
    "        # required for self.add_noise\n",
    "        self.sqrt_alphas_cumprod = self.alphas_cumprod ** 0.5\n",
    "        self.sqrt_one_minus_alphas_cumprod = (1 - self.alphas_cumprod) ** 0.5\n",
    "\n",
    "        # required for reconstruct_x0\n",
    "        self.sqrt_inv_alphas_cumprod = torch.sqrt(1 / self.alphas_cumprod)\n",
    "        self.sqrt_inv_alphas_cumprod_minus_one = torch.sqrt(\n",
    "            1 / self.alphas_cumprod - 1)\n",
    "\n",
    "        # required for q_posterior\n",
    "        self.posterior_mean_coef1 = self.betas * torch.sqrt(self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n",
    "        self.posterior_mean_coef2 = (1. - self.alphas_cumprod_prev) * torch.sqrt(self.alphas) / (1. - self.alphas_cumprod)\n",
    "\n",
    "    def reconstruct_x0(self, x_t, t, noise):\n",
    "        s1 = self.sqrt_inv_alphas_cumprod[t]\n",
    "        s2 = self.sqrt_inv_alphas_cumprod_minus_one[t]\n",
    "        s1 = s1.reshape(-1, 1).to(x_t.device)\n",
    "        s2 = s2.reshape(-1, 1).to(x_t.device)\n",
    "        return s1 * x_t - s2 * noise\n",
    "\n",
    "    def q_posterior(self, x_0, x_t, t):\n",
    "        s1 = self.posterior_mean_coef1[t]\n",
    "        s2 = self.posterior_mean_coef2[t]\n",
    "        s1 = s1.reshape(-1, 1).to(x_t.device)\n",
    "        s2 = s2.reshape(-1, 1).to(x_t.device)\n",
    "        mu = s1 * x_0 + s2 * x_t\n",
    "        return mu\n",
    "\n",
    "    def get_variance(self, t):\n",
    "        if t == 0:\n",
    "            return 0\n",
    "\n",
    "        variance = self.betas[t] * (1. - self.alphas_cumprod_prev[t]) / (1. - self.alphas_cumprod[t])\n",
    "        variance = variance.clip(1e-20)\n",
    "        return variance\n",
    "\n",
    "    def step(self, model_output, timestep, sample):\n",
    "        t = timestep\n",
    "        pred_original_sample = self.reconstruct_x0(sample, t, model_output)\n",
    "        pred_prev_sample = self.q_posterior(pred_original_sample, sample, t)\n",
    "\n",
    "        variance = 0\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(model_output).to(model_output.device)\n",
    "            variance = (self.get_variance(t) ** 0.5) * noise\n",
    "\n",
    "        pred_prev_sample = pred_prev_sample + variance\n",
    "\n",
    "        return pred_original_sample, pred_prev_sample\n",
    "\n",
    "    def step_wgrad(self, model_output, timestep, sample, grad, scale = 0.5):\n",
    "\n",
    "        t = timestep\n",
    "\n",
    "        ## Change\n",
    "        res = (scale * self.sqrt_one_minus_alphas_cumprod[t] * grad.float())\n",
    "        model_output = model_output - (scale * self.sqrt_one_minus_alphas_cumprod[t] * grad.float())\n",
    "\n",
    "        pred_original_sample = self.reconstruct_x0(sample, t, model_output)\n",
    "        pred_prev_sample = self.q_posterior(pred_original_sample, sample, t)\n",
    "\n",
    "        variance = 0\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(model_output).to(model_output.device)\n",
    "            variance = (self.get_variance(t) ** 0.5) * noise\n",
    "\n",
    "        pred_prev_sample = pred_prev_sample + variance\n",
    "\n",
    "        return res, pred_prev_sample\n",
    "\n",
    "    def add_noise(self, x_start, x_noise, timesteps):\n",
    "        s1 = self.sqrt_alphas_cumprod[timesteps]\n",
    "        s2 = self.sqrt_one_minus_alphas_cumprod[timesteps]\n",
    "\n",
    "        s1 = s1.reshape(-1, 1).to(x_start.device)\n",
    "        s2 = s2.reshape(-1, 1).to(x_start.device)\n",
    "\n",
    "        return s1 * x_start + s2 * x_noise\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_timesteps\n",
    "# Define beta schedule\n",
    "T = 1000\n",
    "noise_scheduler = NoiseScheduler(\n",
    "      num_timesteps=T,\n",
    "      beta_schedule=\"linear\")\n",
    "\n",
    "def forward_diffusion_sample(x_0, t):\n",
    "    \"\"\"\n",
    "    Takes an image and a timestep as input and\n",
    "    returns the noisy version of it\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x_0)\n",
    "\n",
    "    return noise_scheduler.add_noise(x_0, noise, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Compute KL Divergence\n",
    "\n",
    "def kl_mvn(m0, S0, m1, S1):\n",
    "    \"\"\"\n",
    "    https://stackoverflow.com/questions/44549369/kullback-leibler-divergence-from-gaussian-pm-pv-to-gaussian-qm-qv\n",
    "    The following function computes the KL-Divergence between any two\n",
    "    multivariate normal distributions\n",
    "    (no need for the covariance matrices to be diagonal)\n",
    "    Kullback-Liebler divergence from Gaussian pm,pv to Gaussian qm,qv.\n",
    "    Also computes KL divergence from a single Gaussian pm,pv to a set\n",
    "    of Gaussians qm,qv.\n",
    "    Diagonal covariances are assumed.  Divergence is expressed in nats.\n",
    "    - accepts stacks of means, but only one S0 and S1\n",
    "    From wikipedia\n",
    "    KL( (m0, S0) || (m1, S1))\n",
    "         = .5 * ( tr(S1^{-1} S0) + log |S1|/|S0| +\n",
    "                  (m1 - m0)^T S1^{-1} (m1 - m0) - N )\n",
    "    # 'diagonal' is [1, 2, 3, 4]\n",
    "    tf.diag(diagonal) ==> [[1, 0, 0, 0]\n",
    "                          [0, 2, 0, 0]\n",
    "                          [0, 0, 3, 0]\n",
    "                          [0, 0, 0, 4]]\n",
    "    # See wikipedia on KL divergence special case.\n",
    "    #KL = 0.5 * tf.reduce_sum(1 + t_log_var - K.square(t_mean) - K.exp(t_log_var), axis=1)\n",
    "                if METHOD['name'] == 'kl_pen':\n",
    "                self.tflam = tf.placeholder(tf.float32, None, 'lambda')\n",
    "                kl = tf.distributions.kl_divergence(oldpi, pi)\n",
    "                self.kl_mean = tf.reduce_mean(kl)\n",
    "                self.aloss = -(tf.reduce_mean(surr - self.tflam * kl))\n",
    "    \"\"\"\n",
    "    # store inv diag covariance of S1 and diff between means\n",
    "    N = m0.shape[0]\n",
    "    try:\n",
    "        iS1 = np.linalg.inv(S1)\n",
    "    except:\n",
    "        print(S1)\n",
    "        print(np.linalg.det(S1))\n",
    "        raise ValueError()\n",
    "\n",
    "    diff = m1 - m0\n",
    "\n",
    "    # kl is made of three terms\n",
    "    tr_term   = np.trace(iS1 @ S0)\n",
    "    det_term  = np.log(np.linalg.det(S1)/np.linalg.det(S0)) #np.sum(np.log(S1)) - np.sum(np.log(S0))\n",
    "    quad_term = diff.T @ np.linalg.inv(S1) @ diff #np.sum( (diff*diff) * iS1, axis=1)\n",
    "    #print(tr_term,det_term,quad_term)\n",
    "    return .5 * (tr_term + det_term + quad_term - N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Compute MMD (maximum mean discrepancy)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def mmd_linear(X, Y):\n",
    "    \"\"\"MMD using linear kernel (i.e., k(x,y) = <x,y>)\n",
    "    Note that this is not the original linear MMD, only the reformulated and faster version.\n",
    "    The original version is:\n",
    "        def mmd_linear(X, Y):\n",
    "            XX = np.dot(X, X.T)\n",
    "            YY = np.dot(Y, Y.T)\n",
    "            XY = np.dot(X, Y.T)\n",
    "            return XX.mean() + YY.mean() - 2 * XY.mean()\n",
    "\n",
    "    Arguments:\n",
    "        X {[n_sample1, dim]} -- [X matrix]\n",
    "        Y {[n_sample2, dim]} -- [Y matrix]\n",
    "\n",
    "    Returns:\n",
    "        [scalar] -- [MMD value]\n",
    "    \"\"\"\n",
    "    delta = X.mean(0) - Y.mean(0)\n",
    "    return delta.dot(delta.T)\n",
    "\n",
    "\n",
    "def mmd_rbf(X, Y, gamma=0.1):\n",
    "    \"\"\"MMD using rbf (gaussian) kernel (i.e., k(x,y) = exp(-gamma * ||x-y||^2 / 2))\n",
    "\n",
    "    Arguments:\n",
    "        X {[n_sample1, dim]} -- [X matrix]\n",
    "        Y {[n_sample2, dim]} -- [Y matrix]\n",
    "\n",
    "    Keyword Arguments:\n",
    "        gamma {float} -- [kernel parameter] (default: {1.0})\n",
    "\n",
    "    Returns:\n",
    "        [scalar] -- [MMD value]\n",
    "    \"\"\"\n",
    "    XX = metrics.pairwise.rbf_kernel(X, X, gamma)\n",
    "    YY = metrics.pairwise.rbf_kernel(Y, Y, gamma)\n",
    "    XY = metrics.pairwise.rbf_kernel(X, Y, gamma)\n",
    "    return XX.mean() + YY.mean() - 2 * XY.mean()\n",
    "\n",
    "\n",
    "def mmd_poly(X, Y, degree=2, gamma=1, coef0=0):\n",
    "    \"\"\"MMD using polynomial kernel (i.e., k(x,y) = (gamma <X, Y> + coef0)^degree)\n",
    "\n",
    "    Arguments:\n",
    "        X {[n_sample1, dim]} -- [X matrix]\n",
    "        Y {[n_sample2, dim]} -- [Y matrix]\n",
    "\n",
    "    Keyword Arguments:\n",
    "        degree {int} -- [degree] (default: {2})\n",
    "        gamma {int} -- [gamma] (default: {1})\n",
    "        coef0 {int} -- [constant item] (default: {0})\n",
    "\n",
    "    Returns:\n",
    "        [scalar] -- [MMD value]\n",
    "    \"\"\"\n",
    "    XX = metrics.pairwise.polynomial_kernel(X, X, degree, gamma, coef0)\n",
    "    YY = metrics.pairwise.polynomial_kernel(Y, Y, degree, gamma, coef0)\n",
    "    XY = metrics.pairwise.polynomial_kernel(X, Y, degree, gamma, coef0)\n",
    "    return XX.mean() + YY.mean() - 2 * XY.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Calculate Frechet Distance (FD)\n",
    "\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "# calculate frechet inception distance\n",
    "def calculate_fid(mu1, sigma1, mu2, sigma2):\n",
    "\n",
    "\t# calculate sum squared difference between means\n",
    "\tssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "\t# calculate sqrt of product between cov\n",
    "\tcovmean = sqrtm(sigma1.dot(sigma2))\n",
    "\t# check and correct imaginary numbers from sqrt\n",
    "\tif np.iscomplexobj(covmean):\n",
    "\t\tcovmean = covmean.real\n",
    "\t# calculate score\n",
    "\tfid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "\n",
    "\treturn fid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments - Non-Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_DATA_FROM_FILE = True\n",
    "\n",
    "if LOAD_DATA_FROM_FILE:\n",
    "    dataX = np.load(f'{OUTPUTS_DIR}/dataX_newset.npy')\n",
    "else:\n",
    "    mu_b_1 = torch.tensor([5.0, 3.0]).to('cuda')\n",
    "    Cov_b_1 = torch.tensor([2.0, 2.0]).to('cuda')\n",
    "\n",
    "    mu_b_2 = torch.tensor([3.0, 7.0]).to('cuda')\n",
    "    Cov_b_2 = torch.tensor([2.0, 2.0]).to('cuda')\n",
    "\n",
    "    mu_b_3 = torch.tensor([7.0, 7.0]).to('cuda')\n",
    "    Cov_b_3 = torch.tensor([2.0, 2.0]).to('cuda')\n",
    "\n",
    "    gmm_b = GaussianMixture([mu_b_1, mu_b_2, mu_b_3],[Cov_b_1, Cov_b_2, Cov_b_3], [1.0, 1.0, 1.0])\n",
    "\n",
    "    dataX, _, _ = gmm_b.sample(3000) # np.random.multivariate_normal([5.0, 5.0], cov, size=3000)\n",
    "\n",
    "    np.save(f'{OUTPUTS_DIR}/dataX_newset.npy', dataX)\n",
    "\n",
    "\n",
    "data_df = pd.DataFrame({'x1': dataX[:, 0], 'x2': dataX[:, 1]})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.kdeplot(data_df, x='x1', y='x2', fill=True, ax=ax, cmap=\"Blues\")\n",
    "ax.scatter(dataX[:, 0], dataX[:, 1], c='b', alpha=0.5, s=10)\n",
    "ax.axis([-2.5,12.5,-2.5,12.5])\n",
    "\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = torch.from_numpy(dataX.astype(np.float32))\n",
    "\n",
    "simpledataset = TensorDataset(data_x) # create your datset\n",
    "dataloader = DataLoader(simpledataset, shuffle=True, batch_size=32) # create your dataloader\n",
    "model = MLP(\n",
    "        hidden_size=128,\n",
    "        hidden_layers=3,\n",
    "        emb_size=128,\n",
    "        time_emb=\"sinusoidal\",\n",
    "        input_emb=\"identity\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=1e-3,\n",
    "    )\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "print(\"Training model...\")\n",
    "\n",
    "progress_bar = tqdm(total=num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "    losses = []\n",
    "    for step, batch in enumerate(dataloader):\n",
    "\n",
    "        # print(batch)\n",
    "\n",
    "        input = batch[0]\n",
    "\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.num_timesteps, (input.shape[0],)\n",
    "        ).long()\n",
    "        noise = torch.randn(input.shape)\n",
    "        noisy = noise_scheduler.add_noise(input, noise, timesteps)\n",
    "        noise_pred = model(noisy, timesteps)\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(loss)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.detach().item())\n",
    "\n",
    "    progress_bar.update(1)\n",
    "    logs = {\"loss\": sum(losses)/len(losses)}\n",
    "    progress_bar.set_postfix(**logs)\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "torch.save(model, f'{OUTPUTS_DIR}/model_uncond_newset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Unconditional sampling\n",
    "\n",
    "export_path = Path(OUTPUTS_DIR).joinpath('uncond')\n",
    "if not Path.exists(export_path):\n",
    "    Path.mkdir(export_path, exist_ok=True, parents=True)\n",
    "\n",
    "model = torch.load(f'{OUTPUTS_DIR}/model_uncond_newset.pt')\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "curr_sample = copy.deepcopy(NOISE_SAMPLES)\n",
    "\n",
    "timesteps = list(range(len(noise_scheduler)))[::-1]\n",
    "for i, t in enumerate(tqdm(timesteps)):\n",
    "    t = torch.from_numpy(np.repeat(t, NUM_SAMPLES)).long()\n",
    "    with torch.no_grad():\n",
    "        residual = model(curr_sample, t)\n",
    "\n",
    "    _, curr_sample = noise_scheduler.step(residual, t[0], curr_sample)\n",
    "\n",
    "curr_sample = curr_sample.detach().cpu()\n",
    "\n",
    "np.save(export_path.joinpath('gensample.npy'), curr_sample.numpy())\n",
    "\n",
    "data_df = pd.DataFrame({'x1': curr_sample[:, 0], 'x2': curr_sample[:, 1]})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.kdeplot(data_df, x='x1', y='x2', fill=True, ax=ax, cmap=\"Blues\")\n",
    "ax.scatter(curr_sample[:, 0], curr_sample[:, 1], c='b', alpha=0.5, s=10)\n",
    "ax.axis([-2.5,12.5,-2.5,12.5])\n",
    "\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(export_path.joinpath('gensample.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Gradient guidance\n",
    "\n",
    "IS_OOD = False\n",
    "\n",
    "if not IS_OOD:\n",
    "    mu_r1 = torch.tensor([5.0, 3.0]).to('cuda')\n",
    "    Cov_r1 = torch.tensor([1.0, 1.0]).to('cuda')\n",
    "\n",
    "    gmm = GaussianMixture([mu_r1],[Cov_r1], [1.0])\n",
    "\n",
    "else:\n",
    "    mu_r2 = torch.tensor([9.0, 3.0]).to('cuda')\n",
    "    Cov_r2 = torch.tensor([2.0, 2.0]).to('cuda')\n",
    "\n",
    "    gmm = GaussianMixture([mu_r2],[Cov_r2], [1.0])\n",
    "\n",
    "model = torch.load(f'{OUTPUTS_DIR}/model_uncond_newset.pt')\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "if not IS_OOD:\n",
    "    export_path = Path(OUTPUTS_DIR).joinpath('gg_id')\n",
    "else:\n",
    "    export_path = Path(OUTPUTS_DIR).joinpath('gg_ood')\n",
    "\n",
    "if not Path.exists(export_path):\n",
    "    Path.mkdir(export_path, exist_ok=True, parents=True)\n",
    "\n",
    "guidance_scales = np.arange(1,20,1).tolist()\n",
    "guidance_scales.extend(np.arange(20, 55, 5).tolist())\n",
    "\n",
    "for guidance_scale in tqdm(guidance_scales, total=len(guidance_scales)):\n",
    "\n",
    "    curr_path = export_path.joinpath(f'scale_{guidance_scale}')\n",
    "    if not Path.exists(curr_path):\n",
    "        Path.mkdir(curr_path, exist_ok=True, parents=True)\n",
    "\n",
    "    curr_sample = copy.deepcopy(NOISE_SAMPLES)\n",
    "\n",
    "    timesteps = list(range(len(noise_scheduler)))[::-1]\n",
    "    for i, t in enumerate(timesteps):\n",
    "        t = torch.from_numpy(np.repeat(t, NUM_SAMPLES)).long()\n",
    "        with torch.no_grad():\n",
    "            residual = model(curr_sample, t)\n",
    "\n",
    "        grad = gmm.score(curr_sample)\n",
    "\n",
    "        res, curr_sample = noise_scheduler.step_wgrad(residual, t[0], curr_sample, grad, scale=guidance_scale)\n",
    "\n",
    "    curr_sample = curr_sample.detach().cpu()\n",
    "\n",
    "    np.save(curr_path.joinpath('gensample.npy'), curr_sample.numpy())\n",
    "\n",
    "    data_df = pd.DataFrame({'x1': curr_sample[:, 0], 'x2': curr_sample[:, 1]})\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    sns.kdeplot(data_df, x='x1', y='x2', fill=True, ax=ax, cmap=\"Blues\")\n",
    "    ax.scatter(curr_sample[:, 0], curr_sample[:, 1], c='b', alpha=0.5, s=10)\n",
    "    ax.axis([0, 10 , 0, 10])\n",
    "\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(curr_path.joinpath('gensample.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Universal guidance\n",
    "\n",
    "IS_OOD = True\n",
    "\n",
    "if not IS_OOD:\n",
    "    mu_r1 = torch.tensor([5.0, 3.0]).to('cuda')\n",
    "    Cov_r1 = torch.tensor([1.0, 1.0]).to('cuda')\n",
    "\n",
    "    gmm = GaussianMixture([mu_r1],[Cov_r1], [1.0])\n",
    "\n",
    "else:\n",
    "    mu_r2 = torch.tensor([9.0, 3.0]).to('cuda')\n",
    "    Cov_r2 = torch.tensor([2.0, 2.0]).to('cuda')\n",
    "\n",
    "    gmm = GaussianMixture([mu_r2],[Cov_r2], [1.0])\n",
    "\n",
    "model = torch.load(f'{OUTPUTS_DIR}/model_uncond_newset.pt')\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "if not IS_OOD:\n",
    "    export_path = Path(OUTPUTS_DIR).joinpath('unvg_id')\n",
    "else:\n",
    "    export_path = Path(OUTPUTS_DIR).joinpath('unvg_ood')\n",
    "\n",
    "if not Path.exists(export_path):\n",
    "    Path.mkdir(export_path, exist_ok=True, parents=True)\n",
    "\n",
    "guidance_scales = np.arange(1,20,1).tolist()\n",
    "guidance_scales.extend(np.arange(20, 55, 5).tolist())\n",
    "\n",
    "rsteps = 1\n",
    "\n",
    "for guidance_scale in tqdm(guidance_scales, total=len(guidance_scales)):\n",
    "\n",
    "    curr_path = export_path.joinpath(f'scale_{guidance_scale}')\n",
    "    if not Path.exists(curr_path):\n",
    "        Path.mkdir(curr_path, exist_ok=True, parents=True)\n",
    "\n",
    "    sample = copy.deepcopy(NOISE_SAMPLES)\n",
    "\n",
    "    timesteps = list(range(len(noise_scheduler)))[::-1]\n",
    "    for i, t in enumerate(tqdm(timesteps)):\n",
    "        t = torch.from_numpy(np.repeat(t, NUM_SAMPLES)).long()\n",
    "\n",
    "        for k in range(rsteps):\n",
    "\n",
    "            with torch.no_grad():\n",
    "                residual = model(sample, t)\n",
    "\n",
    "            residual = copy.deepcopy(residual)\n",
    "\n",
    "            # forward guidance\n",
    "            grad = gmm.score(sample)\n",
    "            residual = residual - (guidance_scale * noise_scheduler.sqrt_one_minus_alphas_cumprod[t[0]] * grad.float())\n",
    "\n",
    "            if True: # SGD\n",
    "\n",
    "                param = copy.deepcopy(sample)\n",
    "                for i in range(1):\n",
    "                    grad_sgd = gmm.score(param)\n",
    "                    param = param + (0.01 * grad_sgd)\n",
    "\n",
    "                deltas = param - sample\n",
    "\n",
    "                # backward guidance\n",
    "                residual = residual - (noise_scheduler.sqrt_alphas_cumprod[t[0]] / noise_scheduler.sqrt_one_minus_alphas_cumprod[t[0]]) * deltas\n",
    "\n",
    "            _, sample = noise_scheduler.step(residual, t[0], sample)\n",
    "\n",
    "        if k < (rsteps - 1):\n",
    "            noise = torch.randn_like(sample).to(sample.device)\n",
    "            sample = (noise_scheduler.sqrt_alphas[t[0]]) * sample + (noise_scheduler.sqrt_betas[t[0]]) * noise\n",
    "\n",
    "    curr_sample = sample.detach().cpu()\n",
    "\n",
    "    np.save(curr_path.joinpath('gensample.npy'), curr_sample.numpy())\n",
    "\n",
    "    data_df = pd.DataFrame({'x1': curr_sample[:, 0], 'x2': curr_sample[:, 1]})\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    sns.kdeplot(data_df, x='x1', y='x2', fill=True, ax=ax, cmap=\"Blues\")\n",
    "    ax.scatter(curr_sample[:, 0], curr_sample[:, 1], c='b', alpha=0.5, s=10)\n",
    "    ax.axis([0, 10 , 0, 10])\n",
    "\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(curr_path.joinpath('gensample.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Controlled Decoding (Block-wise)\n",
    "\n",
    "IS_OOD = True\n",
    "\n",
    "if not IS_OOD:\n",
    "    mu_r1 = torch.tensor([5.0, 3.0]).to('cuda')\n",
    "    Cov_r1 = torch.tensor([1.0, 1.0]).to('cuda')\n",
    "\n",
    "    gmm = GaussianMixture([mu_r1],[Cov_r1], [1.0])\n",
    "\n",
    "else:\n",
    "    mu_r2 = torch.tensor([9.0, 3.0]).to('cuda')\n",
    "    Cov_r2 = torch.tensor([3.0, 3.0]).to('cuda')\n",
    "\n",
    "    gmm = GaussianMixture([mu_r2],[Cov_r2], [1.0])\n",
    "\n",
    "model = torch.load(f'{OUTPUTS_DIR}/model_uncond_newset.pt')\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "bsize = 1\n",
    "\n",
    "if not IS_OOD:\n",
    "    export_path = Path(OUTPUTS_DIR).joinpath(f'CD_b{bsize}_id')\n",
    "else:\n",
    "    export_path = Path(OUTPUTS_DIR).joinpath(f'CD_b{bsize}_ood')\n",
    "\n",
    "if not Path.exists(export_path):\n",
    "    Path.mkdir(export_path, exist_ok=True, parents=True)\n",
    "\n",
    "n_vals = np.arange(2, 20, 2).tolist()\n",
    "n_vals.extend(np.arange(20, 110, 10).tolist())\n",
    "\n",
    "for n_val in tqdm(n_vals, total=len(n_vals)):\n",
    "\n",
    "    curr_path = export_path.joinpath(f'n_{n_val}')\n",
    "    if not Path.exists(curr_path):\n",
    "        Path.mkdir(curr_path, exist_ok=True, parents=True)\n",
    "\n",
    "    curr_sample = copy.deepcopy(NOISE_SAMPLES)\n",
    "    curr_sample = curr_sample.repeat(n_val, 1)\n",
    "\n",
    "    timesteps = list(range(len(noise_scheduler)))[::-1]\n",
    "    for i, t in enumerate(timesteps):\n",
    "        t = torch.from_numpy(np.repeat(t, n_val * NUM_SAMPLES)).long().to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            residual = model(curr_sample, t)\n",
    "\n",
    "        _, curr_sample = noise_scheduler.step(residual, t[0], curr_sample)\n",
    "\n",
    "        if t[0] % bsize == 0: # at the end of block do BoN\n",
    "            \n",
    "            if t[0] > 0: # If not final step use estimates x0\n",
    "                \n",
    "                prev_t = torch.from_numpy(np.repeat(timesteps[i + 1], n_val * NUM_SAMPLES)).long().to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    residual = model(curr_sample, prev_t)\n",
    "\n",
    "                pred_x0 = noise_scheduler.reconstruct_x0(curr_sample, timesteps[i + 1], residual)\n",
    "                reward = gmm.pdf(pred_x0)\n",
    "            else:\n",
    "                reward = gmm.pdf(curr_sample)\n",
    "\n",
    "            reward = torch.cat([x.unsqueeze(0) for x in reward.chunk(n_val)], dim=0)\n",
    "            select_ind = torch.max(reward, dim=0)[1]\n",
    "\n",
    "            gen_sample = copy.deepcopy(curr_sample)\n",
    "            gen_sample = torch.cat([x.unsqueeze(0) for x in gen_sample.chunk(n_val)], dim=0)\n",
    "            gen_sample = gen_sample.permute(1,0,2)\n",
    "            curr_sample = copy.deepcopy(torch.cat([x[select_ind[idx]].unsqueeze(0) for idx, x in enumerate(gen_sample)], dim=0)) # TODO: Make it efficient\n",
    "        \n",
    "            if t[0] > 0: # If not the end replicate n times\n",
    "                curr_sample = curr_sample.repeat(n_val, 1)\n",
    "\n",
    "    # raise ValueError()\n",
    "    curr_sample = curr_sample.detach().cpu()\n",
    "\n",
    "    np.save(curr_path.joinpath('gensample.npy'), curr_sample.numpy())\n",
    "\n",
    "    data_df = pd.DataFrame({'x1': curr_sample[:, 0], 'x2': curr_sample[:, 1]})\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # sns.kdeplot(data_df, x='x1', y='x2', fill=True, ax=ax, cmap=\"Blues\")\n",
    "    ax.scatter(curr_sample[:, 0], curr_sample[:, 1], c='b', alpha=0.5, s=10)\n",
    "    ax.axis([0, 20 , 0, 20])\n",
    "\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(curr_path.joinpath('gensample.png'), dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title BoN Sampling\n",
    "\n",
    "IS_OOD = True\n",
    "\n",
    "if not IS_OOD:\n",
    "    mu_r1 = torch.tensor([5.0, 3.0]).to('cuda')\n",
    "    Cov_r1 = torch.tensor([1.0, 1.0]).to('cuda')\n",
    "\n",
    "    gmm = GaussianMixture([mu_r1],[Cov_r1], [1.0])\n",
    "\n",
    "else:\n",
    "    mu_r2 = torch.tensor([9.0, 3.0]).to('cuda')\n",
    "    Cov_r2 = torch.tensor([2.0, 2.0]).to('cuda')\n",
    "\n",
    "    gmm = GaussianMixture([mu_r2],[Cov_r2], [1.0])\n",
    "\n",
    "model = torch.load(f'{OUTPUTS_DIR}/model_uncond_newset.pt')\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "if not IS_OOD:\n",
    "    export_path = Path(OUTPUTS_DIR).joinpath(f'BoN_id')\n",
    "else:\n",
    "    export_path = Path(OUTPUTS_DIR).joinpath(f'BoN_ood')\n",
    "\n",
    "if not Path.exists(export_path):\n",
    "    Path.mkdir(export_path, exist_ok=True, parents=True)\n",
    "\n",
    "# n_vals = np.arange(2, 20, 2).tolist()\n",
    "# n_vals.extend(np.arange(20, 110, 10).tolist())\n",
    "n_vals = np.arange(200, 400, 50).tolist()\n",
    "\n",
    "for n_val in tqdm(n_vals, total=len(n_vals)):\n",
    "\n",
    "    curr_path = export_path.joinpath(f'n_{n_val}')\n",
    "    if not Path.exists(curr_path):\n",
    "        Path.mkdir(curr_path, exist_ok=True, parents=True)\n",
    "\n",
    "    curr_sample = copy.deepcopy(NOISE_SAMPLES)\n",
    "    curr_sample = curr_sample.repeat(n_val, 1)\n",
    "\n",
    "    timesteps = list(range(len(noise_scheduler)))[::-1]\n",
    "    for i, t in enumerate(timesteps):\n",
    "        t = torch.from_numpy(np.repeat(t, n_val * NUM_SAMPLES)).long().to(DEVICE) ###### Debug\n",
    "        with torch.no_grad():\n",
    "            residual = model(curr_sample, t)\n",
    "\n",
    "        _, curr_sample = noise_scheduler.step(residual, t[0], curr_sample)\n",
    "\n",
    "    np.save(curr_path.joinpath('gen_uncond.npy'), curr_sample.detach().cpu().numpy())\n",
    "\n",
    "    reward = gmm.pdf(curr_sample)\n",
    "    reward = torch.cat([x.unsqueeze(0) for x in reward.chunk(n_val)], dim=0)\n",
    "\n",
    "    # Find the direction that minimizes the loss\n",
    "    select_ind = torch.max(reward, dim=0)[1]\n",
    "    curr_sample = torch.cat([x.unsqueeze(0) for x in curr_sample.chunk(n_val)], dim=0)\n",
    "    curr_sample = curr_sample.permute(1,0,2)\n",
    "    result = copy.deepcopy(torch.cat([x[select_ind[idx]].unsqueeze(0) for idx, x in enumerate(curr_sample)], dim=0)) # TODO: Make it efficient\n",
    "    result = result.detach().cpu()\n",
    "\n",
    "    np.save(curr_path.joinpath('gensample.npy'), result.numpy())\n",
    "\n",
    "    data_df = pd.DataFrame({'x1': result[:, 0], 'x2': result[:, 1]})\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    sns.kdeplot(data_df, x='x1', y='x2', fill=True, ax=ax, cmap=\"Blues\")\n",
    "    ax.scatter(result[:, 0], result[:, 1], c='b', alpha=0.5, s=10)\n",
    "    ax.axis([0, 10 , 0, 10])\n",
    "\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(curr_path.joinpath('gensample.png'), dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title SDEdit\n",
    "\n",
    "IS_OOD = False\n",
    "\n",
    "if not IS_OOD:\n",
    "    mu_r1 = torch.tensor([5.0, 3.0]).to('cuda')\n",
    "    Cov_r1 = torch.tensor([1.0, 1.0]).to('cuda')\n",
    "\n",
    "    gmm = GaussianMixture([mu_r1],[Cov_r1], [1.0])\n",
    "\n",
    "else:\n",
    "    mu_r2 = torch.tensor([9.0, 3.0]).to('cuda')\n",
    "    Cov_r2 = torch.tensor([2.0, 2.0]).to('cuda')\n",
    "\n",
    "    gmm = GaussianMixture([mu_r2],[Cov_r2], [1.0])\n",
    "\n",
    "if not IS_OOD:\n",
    "    export_path = Path(OUTPUTS_DIR).joinpath(f'SDEdit_id')\n",
    "else:\n",
    "    export_path = Path(OUTPUTS_DIR).joinpath(f'SDEdit_ood')\n",
    "\n",
    "if not Path.exists(export_path):\n",
    "    Path.mkdir(export_path, exist_ok=True, parents=True)\n",
    "\n",
    "model = torch.load(f'{OUTPUTS_DIR}/model_uncond_newset.pt')\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# strengths = np.arange(0.4,1.0,0.1).tolist()\n",
    "strengths = np.arange(0.1,1.4,0.1).tolist()\n",
    "\n",
    "target_samples, _, _ = gmm.sample(NUM_SAMPLES)\n",
    "target_samples = torch.from_numpy(target_samples)\n",
    "\n",
    "# print(target_samples.shape)\n",
    "\n",
    "for strength in tqdm(strengths, total=len(strengths)):\n",
    "\n",
    "    curr_path = export_path.joinpath(f'strength_{round(strength * 10)}')\n",
    "    if not Path.exists(curr_path):\n",
    "        Path.mkdir(curr_path, exist_ok=True, parents=True)\n",
    "\n",
    "    curr_sample = copy.deepcopy(target_samples)\n",
    "\n",
    "    curr_sample = forward_diffusion_sample(curr_sample, int(strength*noise_scheduler.num_timesteps))\n",
    "    curr_sample = curr_sample.to(DEVICE)\n",
    "\n",
    "    timesteps = list(range(len(noise_scheduler)))[::-1]\n",
    "    for i, t in enumerate(tqdm(timesteps)):\n",
    "\n",
    "        if t > strength*noise_scheduler.num_timesteps:\n",
    "            continue\n",
    "\n",
    "        t = torch.from_numpy(np.repeat(t, NUM_SAMPLES)).long()\n",
    "        with torch.no_grad():\n",
    "            residual = model(curr_sample, t)\n",
    "\n",
    "        _, curr_sample = noise_scheduler.step(residual, t[0], curr_sample)\n",
    "\n",
    "    curr_sample = curr_sample.detach().cpu()\n",
    "\n",
    "    np.save(curr_path.joinpath('gensample.npy'), curr_sample.numpy())\n",
    "\n",
    "    data_df = pd.DataFrame({'x1': curr_sample[:, 0], 'x2': curr_sample[:, 1]})\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    sns.kdeplot(data_df, x='x1', y='x2', fill=True, ax=ax, cmap=\"Blues\")\n",
    "    ax.scatter(curr_sample[:, 0], curr_sample[:, 1], c='b', alpha=0.5, s=10)\n",
    "    ax.axis([0, 10 , 0, 10])\n",
    "\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(curr_path.joinpath('gensample.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title SDEdit + Gradient guidance\n",
    "\n",
    "IS_OOD = True\n",
    "\n",
    "if not IS_OOD:\n",
    "    mu_r1 = torch.tensor([5.0, 3.0]).to('cuda')\n",
    "    Cov_r1 = torch.tensor([1.0, 1.0]).to('cuda')\n",
    "\n",
    "    gmm = GaussianMixture([mu_r1],[Cov_r1], [1.0])\n",
    "\n",
    "else:\n",
    "    mu_r2 = torch.tensor([9.0, 3.0]).to('cuda')\n",
    "    Cov_r2 = torch.tensor([2.0, 2.0]).to('cuda')\n",
    "\n",
    "    gmm = GaussianMixture([mu_r2],[Cov_r2], [1.0])\n",
    "\n",
    "model = torch.load(f'{OUTPUTS_DIR}/model_uncond_newset.pt')\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "if not IS_OOD:\n",
    "    export_path = Path(OUTPUTS_DIR).joinpath('SDEdit_gg_id')\n",
    "else:\n",
    "    export_path = Path(OUTPUTS_DIR).joinpath('SDEdit_gg_ood')\n",
    "\n",
    "if not Path.exists(export_path):\n",
    "    Path.mkdir(export_path, exist_ok=True, parents=True)\n",
    "\n",
    "strengths = np.arange(0.1,1.0,0.1).tolist()\n",
    "\n",
    "guidance_scales = np.arange(1,20,1).tolist()\n",
    "guidance_scales.extend(np.arange(20, 55, 5).tolist())\n",
    "\n",
    "target_samples, _, _ = gmm.sample(NUM_SAMPLES)\n",
    "target_samples = torch.from_numpy(target_samples)\n",
    "\n",
    "for strength in tqdm(strengths, total=len(strengths)):\n",
    "\n",
    "    for guidance_scale in guidance_scales:\n",
    "\n",
    "        curr_path = export_path.joinpath(f'strength_{round(strength * 10)}_scale_{guidance_scale}')\n",
    "        if not Path.exists(curr_path):\n",
    "            Path.mkdir(curr_path, exist_ok=True, parents=True)\n",
    "\n",
    "        curr_sample = copy.deepcopy(target_samples)\n",
    "\n",
    "        curr_sample = forward_diffusion_sample(curr_sample, int(strength*noise_scheduler.num_timesteps))\n",
    "        curr_sample = curr_sample.to(DEVICE)\n",
    "\n",
    "        timesteps = list(range(len(noise_scheduler)))[::-1]\n",
    "        for i, t in enumerate(timesteps):\n",
    "\n",
    "            if t > strength*noise_scheduler.num_timesteps:\n",
    "                continue\n",
    "\n",
    "            t = torch.from_numpy(np.repeat(t, NUM_SAMPLES)).long()\n",
    "            with torch.no_grad():\n",
    "                residual = model(curr_sample, t)\n",
    "\n",
    "            grad = gmm.score(curr_sample)\n",
    "\n",
    "            res, curr_sample = noise_scheduler.step_wgrad(residual, t[0], curr_sample, grad, scale=guidance_scale)\n",
    "\n",
    "        curr_sample = curr_sample.detach().cpu()\n",
    "\n",
    "        np.save(curr_path.joinpath('gensample.npy'), curr_sample.numpy())\n",
    "\n",
    "        data_df = pd.DataFrame({'x1': curr_sample[:, 0], 'x2': curr_sample[:, 1]})\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        sns.kdeplot(data_df, x='x1', y='x2', fill=True, ax=ax, cmap=\"Blues\")\n",
    "        ax.scatter(curr_sample[:, 0], curr_sample[:, 1], c='b', alpha=0.5, s=10)\n",
    "        ax.axis([0, 10 , 0, 10])\n",
    "\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(curr_path.joinpath('gensample.png'), dpi=300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title SDEdit + Universal guidance\n",
    "\n",
    "IS_OOD = False\n",
    "\n",
    "if not IS_OOD:\n",
    "    mu_r1 = torch.tensor([5.0, 3.0]).to('cuda')\n",
    "    Cov_r1 = torch.tensor([1.0, 1.0]).to('cuda')\n",
    "\n",
    "    gmm = GaussianMixture([mu_r1],[Cov_r1], [1.0])\n",
    "\n",
    "else:\n",
    "    mu_r2 = torch.tensor([9.0, 3.0]).to('cuda')\n",
    "    Cov_r2 = torch.tensor([2.0, 2.0]).to('cuda')\n",
    "\n",
    "    gmm = GaussianMixture([mu_r2],[Cov_r2], [1.0])\n",
    "\n",
    "model = torch.load(f'{OUTPUTS_DIR}/model_uncond_newset.pt')\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "if not IS_OOD:\n",
    "    export_path = Path(OUTPUTS_DIR).joinpath('SDEdit_unvg_id')\n",
    "else:\n",
    "    export_path = Path(OUTPUTS_DIR).joinpath('SDEdit_unvg_ood')\n",
    "\n",
    "if not Path.exists(export_path):\n",
    "    Path.mkdir(export_path, exist_ok=True, parents=True)\n",
    "\n",
    "guidance_scales = np.arange(1,20,1).tolist()\n",
    "guidance_scales.extend(np.arange(20, 55, 5).tolist())\n",
    "\n",
    "rsteps = 1\n",
    "\n",
    "strengths = np.arange(0.1,1.0,0.1).tolist()\n",
    "# strengths = np.arange(0.1,1.4,0.1).tolist()\n",
    "\n",
    "target_samples, _, _ = gmm.sample(NUM_SAMPLES)\n",
    "target_samples = torch.from_numpy(target_samples)\n",
    "\n",
    "for strength in tqdm(strengths, total=len(strengths)):\n",
    "\n",
    "    for guidance_scale in tqdm(guidance_scales, total=len(guidance_scales)):\n",
    "\n",
    "        curr_path = export_path.joinpath(f'strength_{round(strength * 10)}_scale_{guidance_scale}')\n",
    "        if not Path.exists(curr_path):\n",
    "            Path.mkdir(curr_path, exist_ok=True, parents=True)\n",
    "\n",
    "        sample = copy.deepcopy(target_samples)\n",
    "\n",
    "        sample = forward_diffusion_sample(sample, int(strength*noise_scheduler.num_timesteps))\n",
    "        sample = sample.to(DEVICE)\n",
    "\n",
    "        timesteps = list(range(len(noise_scheduler)))[::-1]\n",
    "        for i, t in enumerate(timesteps):\n",
    "\n",
    "            if t > strength*noise_scheduler.num_timesteps:\n",
    "                continue\n",
    "\n",
    "            t = torch.from_numpy(np.repeat(t, NUM_SAMPLES)).long()\n",
    "\n",
    "            for k in range(rsteps):\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    residual = model(sample, t)\n",
    "\n",
    "                residual = copy.deepcopy(residual)\n",
    "\n",
    "                # forward guidance\n",
    "                grad = gmm.score(sample)\n",
    "                residual = residual - (guidance_scale * noise_scheduler.sqrt_one_minus_alphas_cumprod[t[0]] * grad.float())\n",
    "\n",
    "                if True: # SGD\n",
    "\n",
    "                    param = copy.deepcopy(sample)\n",
    "                    for i in range(1):\n",
    "                        grad_sgd = gmm.score(param)\n",
    "                        param = param + (0.01 * grad_sgd)\n",
    "\n",
    "                    deltas = param - sample\n",
    "\n",
    "                    # backward guidance\n",
    "                    residual = residual - (noise_scheduler.sqrt_alphas_cumprod[t[0]] / noise_scheduler.sqrt_one_minus_alphas_cumprod[t[0]]) * deltas\n",
    "\n",
    "                _, sample = noise_scheduler.step(residual, t[0], sample)\n",
    "\n",
    "            if k < (rsteps - 1):\n",
    "                noise = torch.randn_like(sample).to(sample.device)\n",
    "                sample = (noise_scheduler.sqrt_alphas[t[0]]) * sample + (noise_scheduler.sqrt_betas[t[0]]) * noise\n",
    "\n",
    "        curr_sample = sample.detach().cpu()\n",
    "\n",
    "        np.save(curr_path.joinpath('gensample.npy'), curr_sample.numpy())\n",
    "\n",
    "        data_df = pd.DataFrame({'x1': curr_sample[:, 0], 'x2': curr_sample[:, 1]})\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        sns.kdeplot(data_df, x='x1', y='x2', fill=True, ax=ax, cmap=\"Blues\")\n",
    "        ax.scatter(curr_sample[:, 0], curr_sample[:, 1], c='b', alpha=0.5, s=10)\n",
    "        ax.axis([0, 10 , 0, 10])\n",
    "\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(curr_path.joinpath('gensample.png'), dpi=300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title SDEdit + Controlled Decoding (Block-wise)\n",
    "\n",
    "IS_OOD = False\n",
    "\n",
    "if not IS_OOD:\n",
    "    mu_r1 = torch.tensor([5.0, 3.0]).to('cuda')\n",
    "    Cov_r1 = torch.tensor([1.0, 1.0]).to('cuda')\n",
    "\n",
    "    gmm = GaussianMixture([mu_r1],[Cov_r1], [1.0])\n",
    "\n",
    "else:\n",
    "    mu_r2 = torch.tensor([9.0, 3.0]).to('cuda')\n",
    "    Cov_r2 = torch.tensor([2.0, 2.0]).to('cuda')\n",
    "\n",
    "    gmm = GaussianMixture([mu_r2],[Cov_r2], [1.0])\n",
    "\n",
    "model = torch.load(f'{OUTPUTS_DIR}/model_uncond_newset.pt')\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "for bsize in [1, 10, 20, 40, 80, 100]:\n",
    "\n",
    "    if not IS_OOD:\n",
    "        export_path = Path(OUTPUTS_DIR).joinpath(f'SDEdit_CD_b{bsize}_id')\n",
    "    else:\n",
    "        export_path = Path(OUTPUTS_DIR).joinpath(f'SDEdit_CD_b{bsize}_ood')\n",
    "\n",
    "    if not Path.exists(export_path):\n",
    "        Path.mkdir(export_path, exist_ok=True, parents=True)\n",
    "\n",
    "    strengths = np.arange(0.1,1.0,0.1).tolist()\n",
    "\n",
    "    n_vals = np.arange(2, 20, 2).tolist()\n",
    "    n_vals.extend(np.arange(20, 110, 10).tolist())\n",
    "\n",
    "    target_samples, _, _ = gmm.sample(NUM_SAMPLES)\n",
    "    target_samples = torch.from_numpy(target_samples)\n",
    "\n",
    "    for strength in tqdm(strengths, total=len(strengths)):\n",
    "\n",
    "        for n_val in n_vals:\n",
    "\n",
    "            curr_path = export_path.joinpath(f'strength_{round(strength * 10)}_n_{n_val}')\n",
    "            if not Path.exists(curr_path):\n",
    "                Path.mkdir(curr_path, exist_ok=True, parents=True)\n",
    "\n",
    "            curr_sample = copy.deepcopy(target_samples)\n",
    "            curr_sample = forward_diffusion_sample(curr_sample, int(strength*noise_scheduler.num_timesteps))\n",
    "            curr_sample = curr_sample.to(DEVICE)\n",
    "\n",
    "            curr_sample = curr_sample.repeat(n_val, 1)\n",
    "\n",
    "            counter = 0\n",
    "            timesteps = list(range(len(noise_scheduler)))[::-1]\n",
    "            for i, t in enumerate(timesteps):\n",
    "                if t > strength*noise_scheduler.num_timesteps:\n",
    "                    continue\n",
    "\n",
    "                counter += 1\n",
    "        \n",
    "                t = torch.from_numpy(np.repeat(t, n_val * NUM_SAMPLES)).long().to(DEVICE) ###### Debug\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    residual = model(curr_sample, t)\n",
    "\n",
    "                _, curr_sample = noise_scheduler.step(residual, t[0], curr_sample)\n",
    "\n",
    "                if (counter + 1) % bsize == 0 or t[0] == timesteps[-1]: # at the end of block do BoN\n",
    "                    \n",
    "                    if t[0] > timesteps[-1]: # If not final step use estimates x0\n",
    "                        \n",
    "                        prev_t = torch.from_numpy(np.repeat(timesteps[i + 1], n_val * NUM_SAMPLES)).long().to(DEVICE)\n",
    "                        with torch.no_grad():\n",
    "                            residual = model(curr_sample, prev_t)\n",
    "\n",
    "                        pred_x0 = noise_scheduler.reconstruct_x0(curr_sample, timesteps[i + 1], residual)\n",
    "                        reward = gmm.pdf(pred_x0)\n",
    "                    else:\n",
    "                        reward = gmm.pdf(curr_sample)\n",
    "\n",
    "                    reward = torch.cat([x.unsqueeze(0) for x in reward.chunk(n_val)], dim=0)\n",
    "                    select_ind = torch.max(reward, dim=0)[1]\n",
    "\n",
    "                    gen_sample = copy.deepcopy(curr_sample)\n",
    "                    gen_sample = torch.cat([x.unsqueeze(0) for x in gen_sample.chunk(n_val)], dim=0)\n",
    "                    gen_sample = gen_sample.permute(1,0,2)\n",
    "                    curr_sample = copy.deepcopy(torch.cat([x[select_ind[idx]].unsqueeze(0) for idx, x in enumerate(gen_sample)], dim=0)) # TODO: Make it efficient\n",
    "                \n",
    "                    if t[0] > timesteps[-1]: # If not the end replicate n times\n",
    "                        curr_sample = curr_sample.repeat(n_val, 1)\n",
    "\n",
    "            # print(curr_sample.shape)\n",
    "\n",
    "            # raise ValueError()\n",
    "            curr_sample = curr_sample.detach().cpu()\n",
    "\n",
    "            np.save(curr_path.joinpath('gensample.npy'), curr_sample.numpy())\n",
    "\n",
    "            data_df = pd.DataFrame({'x1': curr_sample[:, 0], 'x2': curr_sample[:, 1]})\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "            sns.kdeplot(data_df, x='x1', y='x2', fill=True, ax=ax, cmap=\"Blues\")\n",
    "            ax.scatter(curr_sample[:, 0], curr_sample[:, 1], c='b', alpha=0.5, s=10)\n",
    "            ax.axis([0, 10 , 0, 10])\n",
    "\n",
    "            plt.grid()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(curr_path.joinpath('gensample.png'), dpi=300)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title SDEdit + BoN Sampling\n",
    "\n",
    "IS_OOD = True\n",
    "\n",
    "if not IS_OOD:\n",
    "    mu_r1 = torch.tensor([5.0, 3.0]).to('cuda')\n",
    "    Cov_r1 = torch.tensor([1.0, 1.0]).to('cuda')\n",
    "\n",
    "    gmm = GaussianMixture([mu_r1],[Cov_r1], [1.0])\n",
    "\n",
    "else:\n",
    "    mu_r2 = torch.tensor([9.0, 3.0]).to('cuda')\n",
    "    Cov_r2 = torch.tensor([2.0, 2.0]).to('cuda')\n",
    "\n",
    "    gmm = GaussianMixture([mu_r2],[Cov_r2], [1.0])\n",
    "\n",
    "model = torch.load(f'{OUTPUTS_DIR}/model_uncond_newset.pt')\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "if not IS_OOD:\n",
    "    export_path = Path(OUTPUTS_DIR).joinpath(f'SDEdit_BoN_id')\n",
    "else:\n",
    "    export_path = Path(OUTPUTS_DIR).joinpath(f'SDEdit_BoN_ood')\n",
    "\n",
    "if not Path.exists(export_path):\n",
    "    Path.mkdir(export_path, exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "strengths = np.arange(0.1,0.4,0.1).tolist()\n",
    "\n",
    "n_vals = np.arange(2, 12, 2).tolist()\n",
    "n_vals.extend(np.arange(20, 110, 10).tolist())\n",
    "\n",
    "target_samples, _, _ = gmm.sample(NUM_SAMPLES)\n",
    "target_samples = torch.from_numpy(target_samples)\n",
    "\n",
    "# print(target_samples.shape)\n",
    "\n",
    "for strength in tqdm(strengths, total=len(strengths)):\n",
    "\n",
    "    for n_val in tqdm(n_vals, total=len(n_vals)):\n",
    "\n",
    "        curr_path = export_path.joinpath(f'strength_{round(strength * 10)}_n_{n_val}')\n",
    "        if not Path.exists(curr_path):\n",
    "            Path.mkdir(curr_path, exist_ok=True, parents=True)\n",
    "\n",
    "        curr_sample = copy.deepcopy(target_samples)\n",
    "\n",
    "        curr_sample = forward_diffusion_sample(curr_sample, int(strength*noise_scheduler.num_timesteps))\n",
    "        curr_sample = curr_sample.to(DEVICE)\n",
    "\n",
    "        curr_sample = curr_sample.repeat(n_val, 1)\n",
    "\n",
    "        timesteps = list(range(len(noise_scheduler)))[::-1]\n",
    "        for i, t in enumerate(timesteps):\n",
    "\n",
    "            if t > strength*noise_scheduler.num_timesteps:\n",
    "                continue\n",
    "\n",
    "            t = torch.from_numpy(np.repeat(t, n_val * NUM_SAMPLES)).long().to(DEVICE) ###### Debug\n",
    "            with torch.no_grad():\n",
    "                residual = model(curr_sample, t)\n",
    "\n",
    "            _, curr_sample = noise_scheduler.step(residual, t[0], curr_sample)\n",
    "\n",
    "        np.save(curr_path.joinpath('gen_uncond.npy'), curr_sample.detach().cpu().numpy())\n",
    "\n",
    "        reward = gmm.pdf(curr_sample)\n",
    "        reward = torch.cat([x.unsqueeze(0) for x in reward.chunk(n_val)], dim=0)\n",
    "\n",
    "        # Find the direction that minimizes the loss\n",
    "        select_ind = torch.max(reward, dim=0)[1]\n",
    "        curr_sample = torch.cat([x.unsqueeze(0) for x in curr_sample.chunk(n_val)], dim=0)\n",
    "        curr_sample = curr_sample.permute(1,0,2)\n",
    "        result = copy.deepcopy(torch.cat([x[select_ind[idx]].unsqueeze(0) for idx, x in enumerate(curr_sample)], dim=0)) # TODO: Make it efficient\n",
    "        result = result.detach().cpu()\n",
    "\n",
    "        np.save(curr_path.joinpath('gensample.npy'), result.numpy())\n",
    "\n",
    "        data_df = pd.DataFrame({'x1': result[:, 0], 'x2': result[:, 1]})\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        sns.kdeplot(data_df, x='x1', y='x2', fill=True, ax=ax, cmap=\"Blues\")\n",
    "        ax.scatter(result[:, 0], result[:, 1], c='b', alpha=0.5, s=10)\n",
    "        ax.axis([0, 10 , 0, 10])\n",
    "\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(curr_path.joinpath('gensample.png'), dpi=300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Reward vs Divergence Plots\n",
    "\n",
    "IS_OOD = False\n",
    "\n",
    "if not IS_OOD:\n",
    "    mu_r1 = torch.tensor([5.0, 3.0]).to('cuda')\n",
    "    Cov_r1 = torch.tensor([1.0, 1.0]).to('cuda')\n",
    "\n",
    "    gmm = GaussianMixture([mu_r1],[Cov_r1], [1.0])\n",
    "\n",
    "    setting = 'id'\n",
    "\n",
    "else:\n",
    "    mu_r2 = torch.tensor([9.0, 3.0]).to('cuda')\n",
    "    Cov_r2 = torch.tensor([2.0, 2.0]).to('cuda')\n",
    "\n",
    "    gmm = GaussianMixture([mu_r2],[Cov_r2], [1.0])\n",
    "\n",
    "    setting = 'ood'\n",
    "\n",
    "export_path = Path(OUTPUTS_DIR).joinpath(f'plots_{setting}')\n",
    "if not Path.exists(export_path):\n",
    "    Path.mkdir(export_path, exist_ok=True, parents=True)\n",
    "\n",
    "p_samples = np.load(f'{OUTPUTS_DIR}/uncond/gensample.npy')\n",
    "p_mean = np.mean(p_samples, axis=0)\n",
    "p_cov = np.cov(p_samples.T)\n",
    "\n",
    "rew_p = gmm.pdf(torch.from_numpy(p_samples).to('cuda')).cpu().numpy()\n",
    "\n",
    "results = {\n",
    "    'win_rate': [],\n",
    "    'rewards': [],\n",
    "    'fid': [],\n",
    "    'mmd': [],\n",
    "    'kl': [],\n",
    "    'method': []\n",
    "}\n",
    "\n",
    "#####################\n",
    "# Best-of-K\n",
    "#####################\n",
    "\n",
    "results['rewards'].append(rew_p.mean())\n",
    "results['method'].append('Best-of-N')\n",
    "results['fid'].append(0.0)\n",
    "results['kl'].append(0.0)\n",
    "results['mmd'].append(0.0)\n",
    "results['win_rate'].append(0.5)\n",
    "\n",
    "n_vals = np.arange(2, 12, 2).tolist()\n",
    "n_vals.extend(np.arange(20, 50, 10).tolist())\n",
    "n_vals.extend(np.arange(200, 400, 50).tolist())\n",
    "\n",
    "for i, n in enumerate(n_vals):\n",
    "\n",
    "    pi_samples = np.load(f'{OUTPUTS_DIR}/BoN_{setting}/n_{n}/gensample.npy')\n",
    "    pi_samples_filtered = pi_samples[~np.any(np.isnan(pi_samples),axis=1)]\n",
    "    pi_mean = np.mean(pi_samples_filtered, axis=0)\n",
    "    pi_cov = np.cov(pi_samples_filtered.T)\n",
    "\n",
    "    # p_samples_bok = np.load(f'{OUTPUTS_DIR}/BoN_{setting}/n_{n}/gen_uncond.npy')\n",
    "    # p_mean_bok = np.mean(p_samples_bok, axis=0)\n",
    "    # p_cov_bok = np.cov(p_samples_bok.T)\n",
    "\n",
    "    rew = gmm.pdf(torch.from_numpy(pi_samples_filtered).to('cuda')).cpu().numpy()\n",
    "\n",
    "    results['rewards'].append(rew.mean())\n",
    "    results['method'].append('Best-of-N')\n",
    "    results['fid'].append(calculate_fid(pi_mean, pi_cov, p_mean, p_cov))\n",
    "    results['kl'].append(kl_mvn(pi_mean, pi_cov, p_mean, p_cov))\n",
    "    results['mmd'].append(mmd_rbf(pi_samples, p_samples))\n",
    "    results['win_rate'].append((rew > rew_p[~np.any(np.isnan(pi_samples),axis=1)]).astype(int).sum() / len(rew))\n",
    "\n",
    "#####################\n",
    "# Gradient guidance\n",
    "#####################\n",
    "\n",
    "results['rewards'].append(rew_p.mean())\n",
    "results['method'].append('DPS (Chung, 2023)')\n",
    "results['fid'].append(0.0)\n",
    "results['kl'].append(0.0)\n",
    "results['mmd'].append(0.0)\n",
    "results['win_rate'].append(0.5)\n",
    "\n",
    "# Loop over guidance scale\n",
    "guidance_scales = np.arange(1,20,1).tolist()\n",
    "guidance_scales.extend(np.arange(20, 55, 5).tolist())\n",
    "\n",
    "for i, scale in enumerate(guidance_scales):\n",
    "\n",
    "    pi_samples = np.load(f'{OUTPUTS_DIR}/gg_{setting}/scale_{scale}/gensample.npy')\n",
    "    pi_samples_filtered = pi_samples[~np.any(np.isnan(pi_samples),axis=1)]\n",
    "    pi_mean = np.mean(pi_samples_filtered, axis=0)\n",
    "    pi_cov = np.cov(pi_samples_filtered.T)\n",
    "\n",
    "    rew = gmm.pdf(torch.from_numpy(pi_samples_filtered).to('cuda')).cpu().numpy()\n",
    "\n",
    "    results['rewards'].append(rew.mean())\n",
    "    results['method'].append('DPS (Chung, 2023)')\n",
    "    results['fid'].append(calculate_fid(pi_mean, pi_cov, p_mean, p_cov))\n",
    "    results['kl'].append(kl_mvn(pi_mean, pi_cov, p_mean, p_cov))\n",
    "    results['mmd'].append(mmd_rbf(pi_samples, p_samples))\n",
    "    results['win_rate'].append((rew > rew_p[~np.any(np.isnan(pi_samples),axis=1)]).astype(int).sum() / len(rew))\n",
    "\n",
    "#####################\n",
    "# Universal guidance\n",
    "#####################\n",
    "\n",
    "results['rewards'].append(rew_p.mean())\n",
    "results['method'].append('UG (Bansal, 2024)')\n",
    "results['fid'].append(0.0)\n",
    "results['kl'].append(0.0)\n",
    "results['mmd'].append(0.0)\n",
    "results['win_rate'].append(0.5)\n",
    "\n",
    "# Loop over guidance scale\n",
    "guidance_scales = np.arange(1,20,1).tolist()\n",
    "guidance_scales.extend(np.arange(20, 55, 5).tolist())\n",
    "\n",
    "for i, scale in enumerate(guidance_scales):\n",
    "\n",
    "    pi_samples = np.load(f'{OUTPUTS_DIR}/unvg_{setting}/scale_{scale}/gensample.npy')\n",
    "    pi_samples_filtered = pi_samples[~np.any(np.isnan(pi_samples),axis=1)]\n",
    "    pi_mean = np.mean(pi_samples_filtered, axis=0)\n",
    "    pi_cov = np.cov(pi_samples_filtered.T)\n",
    "\n",
    "    rew = gmm.pdf(torch.from_numpy(pi_samples_filtered).to('cuda')).cpu().numpy()\n",
    "\n",
    "    results['rewards'].append(rew.mean())\n",
    "    results['method'].append('UG (Bansal, 2024)')\n",
    "    results['fid'].append(calculate_fid(pi_mean, pi_cov, p_mean, p_cov))\n",
    "    results['kl'].append(kl_mvn(pi_mean, pi_cov, p_mean, p_cov))\n",
    "    results['mmd'].append(mmd_rbf(pi_samples, p_samples))\n",
    "    results['win_rate'].append((rew > rew_p[~np.any(np.isnan(pi_samples),axis=1)]).astype(int).sum() / len(rew))\n",
    "\n",
    "#######################\n",
    "# CoDe (block-wise)\n",
    "#######################\n",
    "\n",
    "block_sizes = [10, 20, 40, 80]\n",
    "# block_sizes = [80]\n",
    "\n",
    "for blocksize in block_sizes:\n",
    "\n",
    "    results['rewards'].append(rew_p.mean())\n",
    "    results['method'].append(f'CoDe (Ours) [block: {blocksize}]')\n",
    "    results['fid'].append(0.0)\n",
    "    results['kl'].append(0.0)\n",
    "    results['mmd'].append(0.0)\n",
    "    results['win_rate'].append(0.5)\n",
    "\n",
    "    n_vals = np.arange(2, 12, 2).tolist()\n",
    "    n_vals.extend(np.arange(20, 50, 10).tolist())\n",
    "\n",
    "    for i, n in enumerate(n_vals):\n",
    "\n",
    "        pi_samples = np.load(f'{OUTPUTS_DIR}/CD_b{blocksize}_{setting}/n_{n}/gensample.npy')\n",
    "        pi_samples_filtered = pi_samples[~np.any(np.isnan(pi_samples),axis=1)]\n",
    "        pi_mean = np.mean(pi_samples_filtered, axis=0)\n",
    "        pi_cov = np.cov(pi_samples_filtered.T)\n",
    "\n",
    "        rew = gmm.pdf(torch.from_numpy(pi_samples_filtered).to('cuda')).cpu().numpy()\n",
    "\n",
    "        results['rewards'].append(rew.mean())\n",
    "        results['method'].append(f'CoDe (Ours) [block: {blocksize}]')\n",
    "        results['fid'].append(calculate_fid(pi_mean, pi_cov, p_mean, p_cov))\n",
    "        results['kl'].append(kl_mvn(pi_mean, pi_cov, p_mean, p_cov))\n",
    "        results['mmd'].append(mmd_rbf(pi_samples, p_samples))\n",
    "        results['win_rate'].append((rew > rew_p[~np.any(np.isnan(pi_samples),axis=1)]).astype(int).sum() / len(rew))\n",
    "\n",
    "#######################\n",
    "# SVDD-PM\n",
    "#######################\n",
    "\n",
    "results['rewards'].append(rew_p.mean())\n",
    "results['method'].append(f'SVDD-PM (Li, 2024)')\n",
    "results['fid'].append(0.0)\n",
    "results['kl'].append(0.0)\n",
    "results['mmd'].append(0.0)\n",
    "results['win_rate'].append(0.5)\n",
    "\n",
    "n_vals = np.arange(2, 12, 2).tolist()\n",
    "n_vals.extend(np.arange(20, 50, 10).tolist())\n",
    "\n",
    "for i, n in enumerate(n_vals):\n",
    "\n",
    "    pi_samples = np.load(f'{OUTPUTS_DIR}/CD_b1_{setting}/n_{n}/gensample.npy')\n",
    "    pi_samples_filtered = pi_samples[~np.any(np.isnan(pi_samples),axis=1)]\n",
    "    pi_mean = np.mean(pi_samples_filtered, axis=0)\n",
    "    pi_cov = np.cov(pi_samples_filtered.T)\n",
    "\n",
    "    rew = gmm.pdf(torch.from_numpy(pi_samples_filtered).to('cuda')).cpu().numpy()\n",
    "\n",
    "    results['rewards'].append(rew.mean())\n",
    "    results['method'].append(f'SVDD-PM (Li, 2024)')\n",
    "    results['fid'].append(calculate_fid(pi_mean, pi_cov, p_mean, p_cov))\n",
    "    results['kl'].append(kl_mvn(pi_mean, pi_cov, p_mean, p_cov))\n",
    "    results['mmd'].append(mmd_rbf(pi_samples, p_samples))\n",
    "    results['win_rate'].append((rew > rew_p[~np.any(np.isnan(pi_samples),axis=1)]).astype(int).sum() / len(rew))\n",
    "\n",
    "\n",
    "#######################\n",
    "# SDEdit\n",
    "#######################\n",
    "\n",
    "results['rewards'].append(rew_p.mean())\n",
    "results['method'].append('SDEdit (Meng, 2021)')\n",
    "results['fid'].append(0.0)\n",
    "results['kl'].append(0.0)\n",
    "results['mmd'].append(0.0)\n",
    "results['win_rate'].append(0.5)\n",
    "\n",
    "# Loop over strength\n",
    "strengths = np.arange(0.4,1.0,0.1).tolist()\n",
    "\n",
    "for i, strength in enumerate(reversed(strengths)):\n",
    "\n",
    "    pi_samples = np.load(f'{OUTPUTS_DIR}/SDEdit_{setting}/strength_{round(strength * 10)}/gensample.npy')\n",
    "    pi_samples_filtered = pi_samples[~np.any(np.isnan(pi_samples),axis=1)]\n",
    "    pi_mean = np.mean(pi_samples_filtered, axis=0)\n",
    "    pi_cov = np.cov(pi_samples_filtered.T)\n",
    "\n",
    "    rew = gmm.pdf(torch.from_numpy(pi_samples_filtered).to('cuda')).cpu().numpy()\n",
    "\n",
    "    results['rewards'].append(rew.mean())\n",
    "    results['method'].append('SDEdit (Meng, 2021)')\n",
    "    results['fid'].append(calculate_fid(pi_mean, pi_cov, p_mean, p_cov))\n",
    "    results['kl'].append(kl_mvn(pi_mean, pi_cov, p_mean, p_cov))\n",
    "    results['mmd'].append(mmd_rbf(pi_samples, p_samples))\n",
    "    results['win_rate'].append((rew > rew_p[~np.any(np.isnan(pi_samples),axis=1)]).astype(int).sum() / len(rew))\n",
    "\n",
    "#####################\n",
    "# SDEdit + Best-of-K\n",
    "#####################\n",
    "\n",
    "results['rewards'].append(rew_p.mean())\n",
    "results['method'].append('SDEdit + BoN')\n",
    "results['fid'].append(0.0)\n",
    "results['kl'].append(0.0)\n",
    "results['mmd'].append(0.0)\n",
    "results['win_rate'].append(0.5)\n",
    "\n",
    "n_vals = np.arange(2, 12, 2).tolist()\n",
    "n_vals.extend(np.arange(20, 50, 10).tolist())\n",
    "\n",
    "for i, n in enumerate(n_vals):\n",
    "\n",
    "    pi_samples = np.load(f'{OUTPUTS_DIR}/SDEdit_BoN_{setting}/strength_4_n_{n}/gensample.npy')\n",
    "    pi_samples_filtered = pi_samples[~np.any(np.isnan(pi_samples),axis=1)]\n",
    "    pi_mean = np.mean(pi_samples_filtered, axis=0)\n",
    "    pi_cov = np.cov(pi_samples_filtered.T)\n",
    "\n",
    "    # p_samples_bok = np.load(f'{OUTPUTS_DIR}/SDEdit_BoN_{setting}/strength_4_n_{n}/gen_uncond.npy')\n",
    "    # p_mean_bok = np.mean(p_samples_bok, axis=0)\n",
    "    # p_cov_bok = np.cov(p_samples_bok.T)\n",
    "\n",
    "    rew = gmm.pdf(torch.from_numpy(pi_samples_filtered).to('cuda')).cpu().numpy()\n",
    "\n",
    "    results['rewards'].append(rew.mean())\n",
    "    results['method'].append('SDEdit + BoN')\n",
    "    results['fid'].append(calculate_fid(pi_mean, pi_cov, p_mean, p_cov))\n",
    "    results['kl'].append(kl_mvn(pi_mean, pi_cov, p_mean, p_cov))\n",
    "    results['mmd'].append(mmd_rbf(pi_samples, p_samples))\n",
    "    results['win_rate'].append((rew > rew_p[~np.any(np.isnan(pi_samples),axis=1)]).astype(int).sum() / len(rew))\n",
    "\n",
    "#####################\n",
    "# SDEdit + Gradient guidance\n",
    "#####################\n",
    "\n",
    "results['rewards'].append(rew_p.mean())\n",
    "results['method'].append('SDEdit + DPS')\n",
    "results['fid'].append(0.0)\n",
    "results['kl'].append(0.0)\n",
    "results['mmd'].append(0.0)\n",
    "results['win_rate'].append(0.5)\n",
    "\n",
    "# Loop over guidance scale\n",
    "guidance_scales = np.arange(1,20,1).tolist()\n",
    "guidance_scales.extend(np.arange(20, 55, 5).tolist())\n",
    "\n",
    "for i, scale in enumerate(guidance_scales):\n",
    "\n",
    "    pi_samples = np.load(f'{OUTPUTS_DIR}/SDEdit_gg_{setting}/strength_4_scale_{scale}/gensample.npy')\n",
    "    pi_samples_filtered = pi_samples[~np.any(np.isnan(pi_samples),axis=1)]\n",
    "    pi_mean = np.mean(pi_samples_filtered, axis=0)\n",
    "    pi_cov = np.cov(pi_samples_filtered.T)\n",
    "\n",
    "    rew = gmm.pdf(torch.from_numpy(pi_samples_filtered).to('cuda')).cpu().numpy()\n",
    "\n",
    "    results['rewards'].append(rew.mean())\n",
    "    results['method'].append('SDEdit + DPS')\n",
    "    results['fid'].append(calculate_fid(pi_mean, pi_cov, p_mean, p_cov))\n",
    "    results['kl'].append(kl_mvn(pi_mean, pi_cov, p_mean, p_cov))\n",
    "    results['mmd'].append(mmd_rbf(pi_samples, p_samples))\n",
    "    results['win_rate'].append((rew > rew_p[~np.any(np.isnan(pi_samples),axis=1)]).astype(int).sum() / len(rew))\n",
    "\n",
    "#####################\n",
    "# SDEdit + Universal guidance\n",
    "#####################\n",
    "\n",
    "results['rewards'].append(rew_p.mean())\n",
    "results['method'].append('SDEdit + Universal Guidance')\n",
    "results['fid'].append(0.0)\n",
    "results['kl'].append(0.0)\n",
    "results['mmd'].append(0.0)\n",
    "results['win_rate'].append(0.5)\n",
    "\n",
    "# Loop over guidance scale\n",
    "guidance_scales = np.arange(1,20,1).tolist()\n",
    "guidance_scales.extend(np.arange(20, 55, 5).tolist())\n",
    "\n",
    "for i, scale in enumerate(guidance_scales):\n",
    "\n",
    "    pi_samples = np.load(f'{OUTPUTS_DIR}/SDEdit_unvg_{setting}/strength_4_scale_{scale}/gensample.npy')\n",
    "    pi_samples_filtered = pi_samples[~np.any(np.isnan(pi_samples),axis=1)]\n",
    "    pi_mean = np.mean(pi_samples_filtered, axis=0)\n",
    "    pi_cov = np.cov(pi_samples_filtered.T)\n",
    "\n",
    "    rew = gmm.pdf(torch.from_numpy(pi_samples_filtered).to('cuda')).cpu().numpy()\n",
    "\n",
    "    results['rewards'].append(rew.mean())\n",
    "    results['method'].append('SDEdit + Universal Guidance')\n",
    "    results['fid'].append(calculate_fid(pi_mean, pi_cov, p_mean, p_cov))\n",
    "    results['kl'].append(kl_mvn(pi_mean, pi_cov, p_mean, p_cov))\n",
    "    results['mmd'].append(mmd_rbf(pi_samples, p_samples))\n",
    "    results['win_rate'].append((rew > rew_p[~np.any(np.isnan(pi_samples),axis=1)]).astype(int).sum() / len(rew))\n",
    "\n",
    "#######################\n",
    "# SDEdit + CoDe\n",
    "#######################\n",
    "\n",
    "block_sizes = [10, 20, 40, 80]\n",
    "# block_sizes = [80]\n",
    "\n",
    "for blocksize in block_sizes:\n",
    "\n",
    "    results['rewards'].append(rew_p.mean())\n",
    "    results['method'].append(f'SDEdit + CoDe (Ours) [block: {blocksize}]')\n",
    "    results['fid'].append(0.0)\n",
    "    results['kl'].append(0.0)\n",
    "    results['mmd'].append(0.0)\n",
    "    results['win_rate'].append(0.5)\n",
    "\n",
    "    n_vals = np.arange(2, 12, 2).tolist()\n",
    "    n_vals.extend(np.arange(20, 50, 10).tolist())\n",
    "\n",
    "    for i, n in enumerate(n_vals):\n",
    "\n",
    "        pi_samples = np.load(f'{OUTPUTS_DIR}/SDEdit_CD_b{blocksize}_{setting}/strength_4_n_{n}/gensample.npy')\n",
    "        pi_samples_filtered = pi_samples[~np.any(np.isnan(pi_samples),axis=1)]\n",
    "        pi_mean = np.mean(pi_samples_filtered, axis=0)\n",
    "        pi_cov = np.cov(pi_samples_filtered.T)\n",
    "\n",
    "        rew = gmm.pdf(torch.from_numpy(pi_samples_filtered).to('cuda')).cpu().numpy()\n",
    "\n",
    "        results['rewards'].append(rew.mean())\n",
    "        results['method'].append(f'SDEdit + CoDe (Ours) [block: {blocksize}]')\n",
    "        results['fid'].append(calculate_fid(pi_mean, pi_cov, p_mean, p_cov))\n",
    "        results['kl'].append(kl_mvn(pi_mean, pi_cov, p_mean, p_cov))\n",
    "        results['mmd'].append(mmd_rbf(pi_samples, p_samples))\n",
    "        results['win_rate'].append((rew > rew_p[~np.any(np.isnan(pi_samples),axis=1)]).astype(int).sum() / len(rew))\n",
    "\n",
    "################################\n",
    "# SDEdit + SVDD-PM\n",
    "################################\n",
    "\n",
    "results['rewards'].append(rew_p.mean())\n",
    "results['method'].append(f'SDEdit + SVDD-PM')\n",
    "results['fid'].append(0.0)\n",
    "results['kl'].append(0.0)\n",
    "results['mmd'].append(0.0)\n",
    "results['win_rate'].append(0.5)\n",
    "\n",
    "n_vals = np.arange(2, 12, 2).tolist()\n",
    "n_vals.extend(np.arange(20, 50, 10).tolist())\n",
    "\n",
    "for i, n in enumerate(n_vals):\n",
    "\n",
    "    pi_samples = np.load(f'{OUTPUTS_DIR}/SDEdit_CD_b1_{setting}/strength_4_n_{n}/gensample.npy')\n",
    "    pi_samples_filtered = pi_samples[~np.any(np.isnan(pi_samples),axis=1)]\n",
    "    pi_mean = np.mean(pi_samples_filtered, axis=0)\n",
    "    pi_cov = np.cov(pi_samples_filtered.T)\n",
    "\n",
    "    rew = gmm.pdf(torch.from_numpy(pi_samples_filtered).to('cuda')).cpu().numpy()\n",
    "\n",
    "    results['rewards'].append(rew.mean())\n",
    "    results['method'].append(f'SDEdit + SVDD-PM')\n",
    "    results['fid'].append(calculate_fid(pi_mean, pi_cov, p_mean, p_cov))\n",
    "    results['kl'].append(kl_mvn(pi_mean, pi_cov, p_mean, p_cov))\n",
    "    results['mmd'].append(mmd_rbf(pi_samples, p_samples))\n",
    "    results['win_rate'].append((rew > rew_p[~np.any(np.isnan(pi_samples),axis=1)]).astype(int).sum() / len(rew))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results = results_df.loc[results_df['method'].isin([\n",
    "    'Best-of-N', \n",
    "    'DPS (Chung, 2023)',\n",
    "    'UG (Bansal, 2024)', \n",
    "    # 'CoDe (Ours) [block: 10]',\n",
    "    # 'CoDe (Ours) [block: 20]', \n",
    "    # 'CoDe (Ours) [block: 40]',\n",
    "    'CoDe (Ours) [block: 80]',\n",
    "    # 'CoDe Updated (Ours) [block: 10]',\n",
    "    # 'CoDe Updated (Ours) [block: 20]',\n",
    "    # 'CoDe Updated (Ours) [block: 40]',\n",
    "    'CoDe (Ours) [block: 80]', \n",
    "    # 'CoDe Updated (Ours) [block: 1000]',\n",
    "    'SVDD-PM (Li, 2024)',\n",
    "    # 'SDEdit (Meng, 2021)',\n",
    "    # 'SDEdit + BoN',\n",
    "    # 'SDEdit + DPS',\n",
    "    # 'SDEdit + Universal Guidance',\n",
    "    # 'SDEdit + SVDD-MC',\n",
    "    # 'SDEdit + CoDe (Ours) [block: 10]',\n",
    "    # 'SDEdit + CoDe (Ours) [block: 20]',\n",
    "    # 'SDEdit + CoDe Updated (Ours) [block: 10]',\n",
    "    # 'SDEdit + CoDe Updated (Ours) [block: 20]',\n",
    "    # 'SDEdit + CoDe Updated (Ours) [block: 40]',\n",
    "    'SDEdit + CoDe (Ours) [block: 80]',\n",
    "])]\n",
    "\n",
    "method_labels = {\n",
    "    'Best-of-N': 'BoN',\n",
    "    'SDEdit \\+ BoN': 'C-BoN', \n",
    "    r'CoDe.*': 'CoDe (Ours)',\n",
    "    r'SDEdit \\+ CoDe.*': 'C-CoDe (Ours)',\n",
    "}\n",
    "\n",
    "plot_results['method'] = plot_results['method'].replace(regex=method_labels)\n",
    "\n",
    "normalize = True\n",
    "if normalize:\n",
    "    base_rew = rew_p.mean()\n",
    "    plot_results['rewards'] = plot_results['rewards']/base_rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = {\n",
    "    'BoN': '^',\n",
    "    # 'C-BoN': 'o',\n",
    "    'CoDe (Ours)': 'o',\n",
    "    'C-CoDe (Ours)': '^',\n",
    "    # 'CoDe (Ours) [block 5]': 'o',\n",
    "    # 'C-CoDe (Ours) [block 5 r 0.5]': 'D',\n",
    "    # 'C-CoDe (Ours) [block 5 r 0.6]': '^',\n",
    "    # 'C-CoDe (Ours) [block 5 r 0.7]': 'v',\n",
    "    # 'C-CoDe (Ours) [block 5 r 0.8]': 'D',\n",
    "    # 'C-CoDe (Ours) [block 50 r 0.6]': 'D',\n",
    "    'SVDD-PM (Li, 2024)': 'o',\n",
    "    'UG (Bansal, 2024)': 's',\n",
    "    'DPS (Chung, 2023)': '^'\n",
    "}\n",
    "\n",
    "palette = {\n",
    "    'BoN': sns.color_palette(\"Paired\")[5],\n",
    "    # 'C-BoN': sns.color_palette(\"Paired\")[6],\n",
    "    'CoDe (Ours)': sns.color_palette(\"Paired\")[3],\n",
    "    'C-CoDe (Ours)': sns.color_palette(\"Paired\")[1],\n",
    "    # 'CoDe (Ours) [block 5]': sns.color_palette(\"Paired\")[3],\n",
    "    # 'C-CoDe (Ours) [block 50 r 0.6]': sns.color_palette(\"Paired\")[9],\n",
    "    # 'C-CoDe (Ours) [block 5 r 0.6]': sns.color_palette(\"Paired\")[1],\n",
    "    # 'C-CoDe (Ours) [block 5 r 0.7]': sns.color_palette(\"Paired\")[9],\n",
    "    # 'C-CoDe (Ours) [block 5 r 0.8]': sns.color_palette(\"Paired\")[11],\n",
    "    'SVDD-PM (Li, 2024)': sns.color_palette(\"Paired\")[9],\n",
    "    'UG (Bansal, 2024)': sns.color_palette(\"Paired\")[7],\n",
    "    'DPS (Chung, 2023)': sns.color_palette(\"Paired\")[11],\n",
    "}\n",
    "\n",
    "normalize = True\n",
    "labels={\n",
    "    \"rewards\": f\"{'(Normalized)' if normalize else ''} Exp. Rewards\",\n",
    "    \"kl\": \"KL Divergence\",\n",
    "    \"fid\": \"Source FID\",\n",
    "    \"cmmd\": \"Source CMMD\",\n",
    "    \"ref_fid\": \"Reference FID\",\n",
    "    \"ref_cmmd\": \"Reference CMMD\",\n",
    "    \"win_rate\": \"Win Rate\",\n",
    "    \"method\": \"Guidance Method\",\n",
    "    \"clipscore\": \"Text Alignment\",\n",
    "    \"clipwinrate\": \"Win Rate (Text Align)\"\n",
    "}\n",
    "\n",
    "# linestyles = {\n",
    "#     'BoN': '-',\n",
    "#     # 'C-BoN': '-.',\n",
    "#     'CoDe (Ours)': '--',\n",
    "#     'C-CoDe (Ours)': ':',\n",
    "#     'SVDD-MC (Li, 2024)': '--',\n",
    "#     'Universal Guidance (Bansal, 2024)': '-.',\n",
    "#     'DPS (Chung, 2023)': '-.'\n",
    "# }\n",
    "\n",
    "linestyles = {\n",
    "    'BoN': '--',\n",
    "    # 'C-BoN': sns.color_palette(\"Paired\")[6],\n",
    "    'CoDe (Ours)': '-',\n",
    "    'C-CoDe (Ours)': '-',\n",
    "    # 'CoDe (Ours) [block 5]': '--',\n",
    "    # 'C-CoDe (Ours) [block 50 r 0.6]': '-',\n",
    "    # 'C-CoDe (Ours) [block 5 r 0.6]': '-',\n",
    "    # 'C-CoDe (Ours) [block 5 r 0.7]': '-',\n",
    "    # 'C-CoDe (Ours) [block 5 r 0.8]': '-',\n",
    "    'SVDD-PM (Li, 2024)': '-',\n",
    "    'UG (Bansal, 2024)': '--',\n",
    "    'DPS (Chung, 2023)': '-',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for perf in ['rewards', 'win_rate']:\n",
    "#     for div in ['fid', 'mmd', 'kl']:\n",
    "\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "perf = ['rewards', 'win_rate']\n",
    "div = ['kl', 'kl'] \n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "\n",
    "    g = sns.lineplot(plot_results, x=div[i], y=perf[i], hue='method', style='method',\n",
    "                    linestyle='-', markers=markers, palette=palette, markersize=6, fillstyle='none', markeredgecolor=None, markeredgewidth=2,\n",
    "                    style_order=list(palette.keys()), ax=ax, sort=False)\n",
    "\n",
    "    ax.get_legend().remove()\n",
    "\n",
    "    if i == 0:\n",
    "      x1, x2, y1, y2 = 2, 6, 0.8, 1.0  # subregion of the original image\n",
    "    else:\n",
    "      x1, x2, y1, y2 = 1.5, 5.5, 0.9, 1.01  # subregion of the original image\n",
    "\n",
    "    axins = ax.inset_axes(\n",
    "        [0.55, 0.1, .4, .4],\n",
    "        xlim=(x1, x2), ylim=(y1, y2)) #, xticklabels=[], yticklabels=[])\n",
    "    ax.indicate_inset_zoom(axins, edgecolor=\"black\")\n",
    "\n",
    "    g = sns.lineplot(plot_results, x=div[i], y=perf[i], hue='method', style='method',\n",
    "                    linestyle='-', markers=markers, palette=palette, markersize=7, fillstyle='none', markeredgecolor=None, markeredgewidth=2,\n",
    "                    style_order=list(palette.keys()), ax=axins, sort=False, legend=False)\n",
    "      \n",
    "    axins.set_xlabel('')\n",
    "    axins.set_ylabel('')\n",
    "\n",
    "    ax.set_xlabel(labels[div[i]], fontsize=14)\n",
    "    ax.set_ylabel(labels[perf[i]], fontsize=14)\n",
    "    ax.tick_params(axis='both', labelsize=12)\n",
    "\n",
    "handles, l_labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, l_labels, bbox_to_anchor=(0.94, 1.03), ncols=3, fontsize=14, frameon=False)\n",
    "# # plt.show()\n",
    "plt.savefig(export_path.joinpath(f'result_{setting}.png'), dpi=300, bbox_inches='tight')\n",
    "# # plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
